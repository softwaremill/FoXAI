{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Explanation analysis using pneumonia XRay pictures with FoXAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check if you need to reinstall linux libaries related to OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install ffmpeg libsm6 libxext6  -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing most needed libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn foxai tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_eezmSZJJe7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from random import shuffle\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Dataset Exploration and Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this notebook we create dataset from 3 sources: \n",
    "1. [Open dataset of Covid-19 cases with chest X-ray](https://github.com/ieee8023/covid-chestxray-dataset)\n",
    "2. [Paul Mooney's pneumonia dataset from ](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)\n",
    "3. [Kaggle's Covid-19 radiography database](https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "id": "ZghMYdMHMGuj"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./covid-chestxray-dataset/metadata.csv')\n",
    "selected_df = df[df.finding==\"Pneumonia/Viral/COVID-19\"]\n",
    "selected_df = selected_df[(selected_df.view == \"AP\") | (selected_df.view == \"PA\")]\n",
    "selected_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "RADFgzxtKwbj"
   },
   "outputs": [],
   "source": [
    "images = selected_df.filename.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvdZqbdhLQLy"
   },
   "outputs": [],
   "source": [
    "shutil.rmtree('./COVID19-DATASET/train/covid19')\n",
    "os.makedirs('./COVID19-DATASET/train/covid19')\n",
    "shutil.rmtree('./COVID19-DATASET/train/normal')\n",
    "os.makedirs('./COVID19-DATASET/train/normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yLVFTOzMBc4"
   },
   "outputs": [],
   "source": [
    "COVID_PATH = './COVID19-DATASET/train/covid19'\n",
    "NORMAL_PATH = './COVID19-DATASET/train/normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zN4DENFlLW9U"
   },
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    shutil.copy(os.path.join('./covid-chestxray-dataset/images', image), os.path.join(COVID_PATH, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FP1C7Ai0QEnw"
   },
   "outputs": [],
   "source": [
    "for image in os.listdir('chest_xray/train/NORMAL')[:300]:\n",
    "    shutil.copy(os.path.join('chest_xray/train/NORMAL', image), os.path.join(NORMAL_PATH, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_TEST = 'COVID-19_Radiography_Dataset/COVID/images'\n",
    "NORMAL_TEST = 'COVID-19_Radiography_Dataset/Normal/images'\n",
    "\n",
    "for image in os.listdir(COVID_TEST)[:300]:\n",
    "    shutil.copy(os.path.join(COVID_TEST, image), os.path.join('./COVID19-DATASET/train/covid19', image))\n",
    "for image in os.listdir(NORMAL_TEST)[:300]:\n",
    "    shutil.copy(os.path.join(NORMAL_TEST, image), os.path.join('./COVID19-DATASET/train/normal', image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GG1zTPHaRSKS"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = './COVID19-DATASET/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = os.listdir(DATA_PATH)\n",
    "image_count = {}\n",
    "for i in class_names:\n",
    "    count = len(os.listdir(os.path.join(DATA_PATH,i)))\n",
    "    lab = f\"normal ({count})\"\n",
    "    if i == \"covid19\":\n",
    "        lab = f\"pneumonia ({count})\"\n",
    "    image_count[lab] = count\n",
    "\n",
    "#Plotting Distribution of Each Classes\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(image_count.values(),\n",
    "        labels = image_count.keys(),\n",
    "        shadow=True,\n",
    "        autopct = '%1.1f%%',\n",
    "        startangle=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,5))\n",
    "fig.suptitle(\"Pneumonia\", size=22)\n",
    "img_paths = os.listdir(COVID_PATH)\n",
    "shuffle(img_paths)\n",
    "\n",
    "for i,image in enumerate(img_paths[:4]):\n",
    "    img = cv2.imread(os.path.join(COVID_PATH, image))\n",
    "    plt.subplot(1,4, i+1, frameon=False)\n",
    "    plt.imshow(img)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,5))\n",
    "fig.suptitle(\"Pneumonia Negative - Healthy\", size=22)\n",
    "img_paths = os.listdir(NORMAL_PATH)\n",
    "shuffle(img_paths)\n",
    "\n",
    "for i,image in enumerate(img_paths[:4]):\n",
    "    img = cv2.imread(os.path.join(NORMAL_PATH, image))\n",
    "    plt.subplot(1,4, i+1, frameon=False)\n",
    "    plt.imshow(img)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AY4KWL6nfxCw"
   },
   "outputs": [],
   "source": [
    "#Statistics Based on ImageNet Data for Normalisation\n",
    "mean_nums = [0.485, 0.456, 0.406]\n",
    "std_nums = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transforms = {\"train\":transforms.Compose([\n",
    "                                transforms.Resize((150,150)), \n",
    "                                transforms.RandomRotation(10), \n",
    "                                transforms.RandomHorizontalFlip(p=0.4), \n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize(mean = mean_nums, std=std_nums)]), \n",
    "                    \"val\": transforms.Compose([\n",
    "                                transforms.Resize((150,150)),\n",
    "                                transforms.CenterCrop(150), \n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=mean_nums, std = std_nums)\n",
    "                    ])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train and Validation Data Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0I-LSUfhsEu",
    "outputId": "cf98846a-6b01-4af6-b600-c9aa532c9d2b"
   },
   "outputs": [],
   "source": [
    "def load_split_train_val_test(datadir, train_size=0.8):\n",
    "    train_data = datasets.ImageFolder(datadir,       \n",
    "                    transform=data_transforms['train']) #Picks up Image Paths from its respective folders and label them\n",
    "    val_data = datasets.ImageFolder(datadir,\n",
    "                    transform=data_transforms['val'])\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    train_split = int(np.floor(train_size * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_idx, val_idx =  indices[:train_split], indices[train_split:]\n",
    "    \n",
    "    dataset_size = {\"train\":len(train_idx), \"val\": len(val_idx)}\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx) # Sampler for splitting train and val images\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(train_data,\n",
    "                   sampler=train_sampler, batch_size=8) # DataLoader provides data from traininng and validation in batches\n",
    "    \n",
    "    valloader = torch.utils.data.DataLoader(val_data,\n",
    "                   sampler=val_sampler, batch_size=8)\n",
    "    \n",
    "    return trainloader, valloader, dataset_size\n",
    "\n",
    "trainloader, valloader, dataset_size = load_split_train_val_test(DATA_PATH, .2)\n",
    "dataloaders = {\"train\" :trainloader, \"val\": valloader}\n",
    "data_sizes = {x: len(dataloaders[x].sampler) for x in ['train','val']}\n",
    "class_names = trainloader.dataset.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsnUH-EiirEm",
    "outputId": "61a55c78-b8f3-4116-be13-ef8ba509b692"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, size =(30,30), title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = mean_nums\n",
    "    std = std_nums\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.figure(figsize=size)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title, size=30)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "F6p9OIFyRhCk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvSXNkTl2ROh"
   },
   "outputs": [],
   "source": [
    "def CNN_Model(pretrained=True):\n",
    "    model = models.resnet18(pretrained=pretrained) # Returns Defined Densenet model with weights trained on ImageNet\n",
    "    model.fc = nn.Linear(512, len(class_names)) # Overwrites the Classifier layer with custom defined layer for transfer learning\n",
    "    model = model.to(device) # Transfer the Model to GPU if available\n",
    "    return model\n",
    "\n",
    "model = CNN_Model(pretrained=True)\n",
    "\n",
    "# specify loss function (categorical cross-entropy loss)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# Specify optimizer which performs Gradient Descent\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "exp_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(trainloader)*40) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we haven't froze the CNN layer parameters untrainable, we are going to train a huge number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9Jb_lED40mH",
    "outputId": "70c2e152-cd5b-4332-9d2d-f85864ab00c4"
   },
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of trainable parameters: \\n{}\".format(pytorch_total_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crbKfZOn5uSx"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            current_loss = 0.0\n",
    "            current_corrects = 0\n",
    "            current_kappa = 0\n",
    "\n",
    "            for inputs, labels in tqdm.tqdm(dataloaders[phase], desc=phase, leave=False):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # We need to zero the gradients in the Cache.\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Time to carry out the forward training poss\n",
    "                # We only need to log the loss stats if we are in training phase\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                # We want variables to hold the loss statistics\n",
    "                current_loss += loss.item() * inputs.size(0)\n",
    "                current_corrects += torch.sum(preds == labels.data)\n",
    "            epoch_loss = current_loss / data_sizes[phase]\n",
    "            epoch_acc = current_corrects.double() / data_sizes[phase]\n",
    "            if phase == 'val':\n",
    "                print('{} Loss: {:.4f} | {} Accuracy: {:.4f}'.format(\n",
    "                    phase, epoch_loss, phase, epoch_acc))\n",
    "            else:\n",
    "                print('{} Loss: {:.4f} | {} Accuracy: {:.4f}'.format(\n",
    "                    phase, epoch_loss, phase, epoch_acc))\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print('Val loss Decreased from {:.4f} to {:.4f} \\nSaving Weights... '.format(best_loss, epoch_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_since = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_since // 60, time_since % 60))\n",
    "    print('Best val loss: {:.4f}'.format(best_loss))\n",
    "\n",
    "    # Now we'll load in the best model weights and return it\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPW9rQGJPtWY",
    "outputId": "5800afdd-59eb-4903-dc4a-97b35752b1f8"
   },
   "outputs": [],
   "source": [
    "base_model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in depth analysis of validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in tqdm.tqdm(valloader, leave=False):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        y_test_pred = base_model(x_batch)\n",
    "        y_test_pred = torch.log_softmax(y_test_pred, dim=1)\n",
    "        _, y_pred_tag = torch.max(y_test_pred, dim = 1)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_true_list.append(y_batch.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [i[0] for i in y_pred_list]\n",
    "y_true_list = [i[0] for i in y_true_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm =  confusion_matrix(y_true_list, y_pred_list)\n",
    "\n",
    "plot_confusion_matrix(cm = cm, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['pneumonia','normal'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testset results:**\n",
    "- VGG19 93% test\n",
    "- VGG11: 93,5%\n",
    "- Resnet34: 93%\n",
    "- Resnet18: 94%\n",
    "- Resnet50: 94,25%\n",
    "- Mobilenetv2: 92,25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('./COVID19-DATASET/test/covid19')\n",
    "os.makedirs('./COVID19-DATASET/test/covid19')\n",
    "shutil.rmtree('./COVID19-DATASET/test/normal')\n",
    "os.makedirs('./COVID19-DATASET/test/normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls COVID-19_Radiography_Dataset/COVID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the purpose of the testset i take 200 last examples from dataset (before we took only first 300 examples while whole dataset is 1200 examples per class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEWvPBn3MAsJ"
   },
   "outputs": [],
   "source": [
    "COVID_TEST = 'COVID-19_Radiography_Dataset/COVID/images'\n",
    "NORMAL_TEST = 'COVID-19_Radiography_Dataset/Normal/images'\n",
    "\n",
    "for image in os.listdir(COVID_TEST)[-200:]:\n",
    "    shutil.copy(os.path.join(COVID_TEST, image), os.path.join('./COVID19-DATASET/test/covid19', image))\n",
    "for image in os.listdir(NORMAL_TEST)[-200:]:\n",
    "    shutil.copy(os.path.join(NORMAL_TEST, image), os.path.join('./COVID19-DATASET/test/normal', image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading testdataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYAcMYROHhiv"
   },
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = './COVID19-DATASET/test/'\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                                      transforms.Resize((150,150)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=mean_nums, std=std_nums)\n",
    "])\n",
    "\n",
    "\n",
    "test_image = datasets.ImageFolder(TEST_DATA_PATH, transform=test_transforms)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_image, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix on the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ff-vTLmcS2VG",
    "outputId": "957c9dd3-3c2d-46d0-ac5a-4f31a7f95ada"
   },
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in tqdm.tqdm(testloader, leave=False):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        y_test_pred = base_model(x_batch)\n",
    "        y_test_pred = torch.log_softmax(y_test_pred, dim=1)\n",
    "        _, y_pred_tag = torch.max(y_test_pred, dim = 1)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_true_list.append(y_batch.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tL9SAsUwLQ5x"
   },
   "outputs": [],
   "source": [
    "y_pred_list = [i[0] for i in y_pred_list]\n",
    "y_true_list = [i[0] for i in y_true_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyptU5r-ZNiL",
    "outputId": "7000543c-8f87-4844-9d80-be40651c183f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm =  confusion_matrix(y_true_list, y_pred_list)\n",
    "\n",
    "plot_confusion_matrix(cm = cm, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['pneumonia','normal'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(base_model.state_dict(), './best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyze here how the model works form perspective of good explanations (where model and label agree) and where it does not "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# function to enable displaying matplotlib Figures in notebooks\n",
    "def show_figure(fig): \n",
    "    dummy = plt.figure()\n",
    "    new_manager = dummy.canvas.manager\n",
    "    new_manager.canvas.figure = fig\n",
    "    new_manager.set_window_title(\"Test\")\n",
    "    fig.set_canvas(new_manager.canvas)\n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foxai imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from foxai.context_manager import FoXaiExplainer, ExplainerWithParams, CVClassificationExplainers\n",
    "from foxai.visualizer import mean_channels_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaner configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"pneumonia\", \"normal\"]\n",
    "layer = [module for module in model.modules() if isinstance(module, torch.nn.Conv2d)][-1]\n",
    "explainer_list = [\n",
    "    ExplainerWithParams(explainer_name=CVClassificationExplainers.CV_LAYER_GRADCAM_EXPLAINER, layer=layer),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Check where model agrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counter = 0\n",
    "max_samples_explained: int = 40\n",
    "\n",
    "# iterate over dataloader\n",
    "for sample_batch in testloader:\n",
    "    sample_list, label_list = sample_batch\n",
    "    # iterate over all samples in batch\n",
    "    for sample, label in zip(sample_list, label_list):\n",
    "        # add batch size dimension to the data sample\n",
    "        input_data = sample.reshape(1, sample.shape[0], sample.shape[1], sample.shape[2]).to(device)\n",
    "        category_name = categories[label.item()]\n",
    "        with FoXaiExplainer(\n",
    "            model=model,\n",
    "            explainers=[ExplainerWithParams(explainer_name=CVClassificationExplainers.CV_LAYER_GRADCAM_EXPLAINER, layer=layer),],\n",
    "            target=label,\n",
    "        ) as xai_model:\n",
    "            # calculate attributes for every explainer\n",
    "            probs, attributes_dict = xai_model(input_data)\n",
    "        if categories[torch.argmax(_).detach().cpu()] == category_name:\n",
    "            for key, value in attributes_dict.items():\n",
    "\n",
    "                # create figure from attributes and original image           \n",
    "                figure = mean_channels_visualization(attributions=value[0], transformed_img=sample, title= f\"Mean of channels ({key})\")\n",
    "                # save figure to artifact directory\n",
    "                title = categories[torch.argmax(_).detach().cpu()] + \", (\" + category_name +\")\"\n",
    "                imshow(sample, (8,8), title=title)\n",
    "                show_figure(figure)\n",
    "\n",
    "            sample_counter += 1\n",
    "        # if we processed desired number of samples break the loop\n",
    "        if sample_counter > max_samples_explained:\n",
    "            break\n",
    "\n",
    "    # if we processed desired number of samples break the loop\n",
    "    if sample_counter > max_samples_explained:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Check where model disagrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counter = 0\n",
    "max_samples_explained: int = 50\n",
    "\n",
    "# iterate over dataloader\n",
    "for sample_batch in testloader:\n",
    "    sample_list, label_list = sample_batch\n",
    "    # iterate over all samples in batch\n",
    "    for sample, label in zip(sample_list, label_list):\n",
    "        # add batch size dimension to the data sample\n",
    "        input_data = sample.reshape(1, sample.shape[0], sample.shape[1], sample.shape[2]).to(device)\n",
    "        category_name = categories[label.item()]\n",
    "        with FoXaiExplainer(\n",
    "            model=model,\n",
    "            explainers=explainer_list,\n",
    "            target=label,\n",
    "        ) as xai_model:\n",
    "            # calculate attributes for every explainer\n",
    "            probs, attributes_dict = xai_model(input_data)\n",
    "        if categories[torch.argmax(_).detach().cpu()] != category_name:\n",
    "            for key, value in attributes_dict.items():\n",
    "\n",
    "                # create figure from attributes and original image           \n",
    "                figure = mean_channels_visualization(attributions=value[0], transformed_img=sample, title= f\"Mean of channels ({key})\")\n",
    "                # save figure to artifact directory\n",
    "                title = categories[torch.argmax(_).detach().cpu()] + \", (\" + category_name +\")\"\n",
    "                imshow(sample, (8,8), title=title)\n",
    "                show_figure(figure)\n",
    "\n",
    "            sample_counter += 1\n",
    "        # if we processed desired number of samples break the loop\n",
    "        if sample_counter > max_samples_explained:\n",
    "            break\n",
    "\n",
    "    # if we processed desired number of samples break the loop\n",
    "    if sample_counter > max_samples_explained:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
