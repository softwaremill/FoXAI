{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Explanation analysis using pneumonia XRay pictures with FoXAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check if you need to reinstall linux libaries related to OpenCV. If errors appears you might consider runing it with ``sudo`` access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install ffmpeg libsm6 libxext6  -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing most needed libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn foxai tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports\n",
    "Adding all the most important imports from torch, matplotlib, sklearn, PIL, opencv. All these will be used in later stages of the notebook for either file management, training the models or visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_eezmSZJJe7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from random import shuffle\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Dataset Exploration and Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this notebook we create dataset from 3 sources: \n",
    "1. [Open dataset of Covid-19 cases with chest X-ray](https://github.com/ieee8023/covid-chestxray-dataset)\n",
    "2. [Paul Mooney's pneumonia dataset ](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)\n",
    "3. [Kaggle's Covid-19 radiography database](https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database)\n",
    "It is assumed that you download them in this directory. \n",
    "\n",
    "#### How to download them?\n",
    "\n",
    "1. **Open dataset of covid-10 cases with chest X-ray**: this you can download via command: \n",
    "\n",
    "```\n",
    "git clone https://github.com/ieee8023/covid-chestxray-dataset.git\n",
    "```\n",
    "\n",
    "2. **Paul Mooney's pneumonia dataset**: this dataset can be downloaded via Kaggle API: \n",
    "\n",
    "```\n",
    "kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
    "```\n",
    "\n",
    "3. **Kaggle's Covid-19 radiography database**: this dataset can be downloaded via Kaggle API: \n",
    "\n",
    "```\n",
    "kaggle datasets download -d tawsifurrahman/covid19-radiography-database\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's explore a bit our dataset No 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "id": "ZghMYdMHMGuj"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./covid-chestxray-dataset/metadata.csv')\n",
    "selected_df = df[df.finding==\"Pneumonia/Viral/COVID-19\"]\n",
    "selected_df = selected_df[(selected_df.view == \"AP\") | (selected_df.view == \"PA\")]\n",
    "selected_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "RADFgzxtKwbj"
   },
   "outputs": [],
   "source": [
    "images = selected_df.filename.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we want to create our dataset directory. As said before \"Our dataset\" will be combination of few datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvdZqbdhLQLy"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "COVID_PATH = './COVID19-DATASET/train/covid19'\n",
    "NORMAL_PATH = './COVID19-DATASET/train/normal'\n",
    "\n",
    "covid_train_path = Path(COVID_PATH)\n",
    "if covid_train_path.exists() and covid_train_path.is_dir(): \n",
    "    shutil.rmtree(covid_train_path)\n",
    "os.makedirs(covid_train_path)\n",
    "\n",
    "normal_train_path = Path(NORMAL_PATH)\n",
    "if normal_train_path.exists() and normal_train_path.is_dir(): \n",
    "    shutil.rmtree(normal_train_path)\n",
    "os.makedirs(normal_train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and copy pictures of covid from dataset No 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zN4DENFlLW9U"
   },
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    shutil.copy(os.path.join('./covid-chestxray-dataset/images', image), os.path.join(COVID_PATH, image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus:** you can add extra pictures (+4300 pictures) of covid from dataset No 2 (from train and test directories), but be aware that they might be of lower labeling quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image in os.listdir('chest_xray/train/PNEUMONIA'):\n",
    "#     shutil.copy(os.path.join('chest_xray/train/PNEUMONIA', image), os.path.join(COVID_PATH, image))\n",
    "    \n",
    "# for image in os.listdir('chest_xray/test/PNEUMONIA'):\n",
    "#     shutil.copy(os.path.join('chest_xray/test/PNEUMONIA', image), os.path.join(COVID_PATH, image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add pictures of normal lungs from dataset No 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FP1C7Ai0QEnw"
   },
   "outputs": [],
   "source": [
    "for image in os.listdir('chest_xray/train/NORMAL')[:300]:\n",
    "    shutil.copy(os.path.join('chest_xray/train/NORMAL', image), os.path.join(NORMAL_PATH, image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add examples from dataset No 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_TEST = 'COVID-19_Radiography_Dataset/COVID/images'\n",
    "NORMAL_TEST = 'COVID-19_Radiography_Dataset/Normal/images'\n",
    "\n",
    "for image in os.listdir(COVID_TEST)[:300]:\n",
    "    shutil.copy(os.path.join(COVID_TEST, image), os.path.join('./COVID19-DATASET/train/covid19', image))\n",
    "for image in os.listdir(NORMAL_TEST)[:300]:\n",
    "    shutil.copy(os.path.join(NORMAL_TEST, image), os.path.join('./COVID19-DATASET/train/normal', image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize proportions in train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GG1zTPHaRSKS"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = './COVID19-DATASET/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = os.listdir(DATA_PATH)\n",
    "image_count = {}\n",
    "for i in class_names:\n",
    "    count = len(os.listdir(os.path.join(DATA_PATH,i)))\n",
    "    lab = f\"normal ({count})\"\n",
    "    if i == \"covid19\":\n",
    "        lab = f\"pneumonia ({count})\"\n",
    "    image_count[lab] = count\n",
    "\n",
    "#Plotting Distribution of Each Classes\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(image_count.values(),\n",
    "        labels = image_count.keys(),\n",
    "        shadow=True,\n",
    "        autopct = '%1.1f%%',\n",
    "        startangle=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to explore some examples in the dataset. Let's start with pneumonia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,5))\n",
    "fig.suptitle(\"Pneumonia\", size=22)\n",
    "img_paths = os.listdir(COVID_PATH)\n",
    "shuffle(img_paths)\n",
    "\n",
    "for i,image in enumerate(img_paths[:4]):\n",
    "    img = cv2.imread(os.path.join(COVID_PATH, image))\n",
    "    plt.subplot(1,4, i+1, frameon=False)\n",
    "    plt.imshow(img)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the healthy examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,5))\n",
    "fig.suptitle(\"Pneumonia Negative - Healthy\", size=22)\n",
    "img_paths = os.listdir(NORMAL_PATH)\n",
    "shuffle(img_paths)\n",
    "\n",
    "for i,image in enumerate(img_paths[:4]):\n",
    "    img = cv2.imread(os.path.join(NORMAL_PATH, image))\n",
    "    plt.subplot(1,4, i+1, frameon=False)\n",
    "    plt.imshow(img)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Transforms\n",
    "Here we define transforms for our training set. We will resize images to 150x150, perform random rotation from -10 to 10 degrees, random horizontal flip). For validation set we will use also resizing and center cropping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AY4KWL6nfxCw"
   },
   "outputs": [],
   "source": [
    "#Statistics Based on ImageNet Data for Normalisation\n",
    "mean_nums = [0.485, 0.456, 0.406]\n",
    "std_nums = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transforms = {\"train\":transforms.Compose([\n",
    "                                transforms.Resize((150,150)), \n",
    "                                transforms.RandomRotation(10), \n",
    "                                transforms.RandomHorizontalFlip(p=0.4), \n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize(mean = mean_nums, std=std_nums)]), \n",
    "                    \"val\": transforms.Compose([\n",
    "                                transforms.Resize((150,150)),\n",
    "                                transforms.CenterCrop(150), \n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=mean_nums, std = std_nums)\n",
    "                    ])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train and Validation Data Split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our train and validation split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0I-LSUfhsEu",
    "outputId": "cf98846a-6b01-4af6-b600-c9aa532c9d2b"
   },
   "outputs": [],
   "source": [
    "def load_split_train_val_test(datadir, train_size=0.8):\n",
    "    train_data = datasets.ImageFolder(datadir,       \n",
    "                    transform=data_transforms['train']) #Picks up Image Paths from its respective folders and label them\n",
    "    val_data = datasets.ImageFolder(datadir,\n",
    "                    transform=data_transforms['val'])\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    train_split = int(np.floor(train_size * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_idx, val_idx =  indices[:train_split], indices[train_split:]\n",
    "    \n",
    "    dataset_size = {\"train\":len(train_idx), \"val\": len(val_idx)}\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx) # Sampler for splitting train and val images\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(train_data,\n",
    "                   sampler=train_sampler, batch_size=8) # DataLoader provides data from traininng and validation in batches\n",
    "    \n",
    "    valloader = torch.utils.data.DataLoader(val_data,\n",
    "                   sampler=val_sampler, batch_size=8)\n",
    "    \n",
    "    return trainloader, valloader, dataset_size\n",
    "\n",
    "trainloader, valloader, dataset_size = load_split_train_val_test(DATA_PATH, .2)\n",
    "dataloaders = {\"train\" :trainloader, \"val\": valloader}\n",
    "data_sizes = {x: len(dataloaders[x].sampler) for x in ['train','val']}\n",
    "class_names = trainloader.dataset.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize one training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsnUH-EiirEm",
    "outputId": "61a55c78-b8f3-4116-be13-ef8ba509b692"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, size =(30,30), title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = mean_nums\n",
    "    std = std_nums\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.figure(figsize=size)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title, size=30)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "F6p9OIFyRhCk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we some further configuration and define model's architecture, optimzer and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvSXNkTl2ROh"
   },
   "outputs": [],
   "source": [
    "def CNN_Model(pretrained=True):\n",
    "    model = models.resnet18(pretrained=pretrained) # Returns Defined Densenet model with weights trained on ImageNet\n",
    "    model.fc = nn.Linear(512, len(class_names)) # Overwrites the Classifier layer with custom defined layer for transfer learning\n",
    "    model = model.to(device) # Transfer the Model to GPU if available\n",
    "    return model\n",
    "\n",
    "model = CNN_Model(pretrained=True)\n",
    "\n",
    "# specify loss function (categorical cross-entropy loss)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# Specify optimizer which performs Gradient Descent\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "exp_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(trainloader)*40) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define train method and run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crbKfZOn5uSx"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            current_loss = 0.0\n",
    "            current_corrects = 0\n",
    "            current_kappa = 0\n",
    "\n",
    "            for inputs, labels in tqdm.tqdm(dataloaders[phase], desc=phase, leave=False):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # We need to zero the gradients in the Cache.\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Time to carry out the forward training poss\n",
    "                # We only need to log the loss stats if we are in training phase\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                # We want variables to hold the loss statistics\n",
    "                current_loss += loss.item() * inputs.size(0)\n",
    "                current_corrects += torch.sum(preds == labels.data)\n",
    "            epoch_loss = current_loss / data_sizes[phase]\n",
    "            epoch_acc = current_corrects.double() / data_sizes[phase]\n",
    "            if phase == 'val':\n",
    "                print('{} Loss: {:.4f} | {} Accuracy: {:.4f}'.format(\n",
    "                    phase, epoch_loss, phase, epoch_acc))\n",
    "            else:\n",
    "                print('{} Loss: {:.4f} | {} Accuracy: {:.4f}'.format(\n",
    "                    phase, epoch_loss, phase, epoch_acc))\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print('Val loss Decreased from {:.4f} to {:.4f} \\nSaving Weights... '.format(best_loss, epoch_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_since = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_since // 60, time_since % 60))\n",
    "    print('Best val loss: {:.4f}'.format(best_loss))\n",
    "\n",
    "    # Now we'll load in the best model weights and return it\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPW9rQGJPtWY",
    "outputId": "5800afdd-59eb-4903-dc4a-97b35752b1f8"
   },
   "outputs": [],
   "source": [
    "base_model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform analysis of our results. \n",
    "\n",
    "First we get results from labels and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in tqdm.tqdm(valloader, leave=False):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        y_test_pred = base_model(x_batch)\n",
    "        y_test_pred = torch.log_softmax(y_test_pred, dim=1)\n",
    "        _, y_pred_tag = torch.max(y_test_pred, dim = 1)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_true_list.append(y_batch.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [i[0] for i in y_pred_list]\n",
    "y_true_list = [i[0] for i in y_true_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define custom function to visualize confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm =  confusion_matrix(y_true_list, y_pred_list)\n",
    "\n",
    "plot_confusion_matrix(cm = cm, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['pneumonia','normal'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testset results:**\n",
    "- VGG19 93% test\n",
    "- VGG11: 93,5%\n",
    "- Resnet34: 93%\n",
    "- Resnet18: 94%\n",
    "- Resnet50: 94,25%\n",
    "- Mobilenetv2: 92,25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to create a testset from different examples than the trainset, consisting of examples from dataset No2 and Dataset No 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_test_path = Path('./COVID19-DATASET/test/covid19')\n",
    "if covid_test_path.exists() and covid_test_path.is_dir():\n",
    "    shutil.rmtree(covid_test_path)\n",
    "os.makedirs(covid_test_path)\n",
    "\n",
    "normal_test_path = Path('./COVID19-DATASET/test/normal')\n",
    "if normal_test_path.exists() and normal_test_path.is_dir():\n",
    "    shutil.rmtree(normal_test_path)\n",
    "os.makedirs('./COVID19-DATASET/test/normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls COVID-19_Radiography_Dataset/COVID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of the testset i take 200 last examples from dataset (before we took only first 300 examples while whole dataset is 1200 examples per class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEWvPBn3MAsJ"
   },
   "outputs": [],
   "source": [
    "COVID_TEST = 'COVID-19_Radiography_Dataset/COVID/images'\n",
    "NORMAL_TEST = 'COVID-19_Radiography_Dataset/Normal/images'\n",
    "\n",
    "for image in os.listdir(COVID_TEST)[-200:]:\n",
    "    shutil.copy(os.path.join(COVID_TEST, image), os.path.join('./COVID19-DATASET/test/covid19', image))\n",
    "for image in os.listdir(NORMAL_TEST)[-200:]:\n",
    "    shutil.copy(os.path.join(NORMAL_TEST, image), os.path.join('./COVID19-DATASET/test/normal', image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading testdataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYAcMYROHhiv"
   },
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = './COVID19-DATASET/test/'\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                                      transforms.Resize((150,150)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=mean_nums, std=std_nums)\n",
    "])\n",
    "\n",
    "\n",
    "test_image = datasets.ImageFolder(TEST_DATA_PATH, transform=test_transforms)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_image, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix on the testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here same as before for validation set and trainset we firstly calculate the predictions and labels from testset and then visualize the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ff-vTLmcS2VG",
    "outputId": "957c9dd3-3c2d-46d0-ac5a-4f31a7f95ada"
   },
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in tqdm.tqdm(testloader, leave=False):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        y_test_pred = base_model(x_batch)\n",
    "        y_test_pred = torch.log_softmax(y_test_pred, dim=1)\n",
    "        _, y_pred_tag = torch.max(y_test_pred, dim = 1)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_true_list.append(y_batch.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tL9SAsUwLQ5x"
   },
   "outputs": [],
   "source": [
    "y_pred_list = [i[0] for i in y_pred_list]\n",
    "y_true_list = [i[0] for i in y_true_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyptU5r-ZNiL",
    "outputId": "7000543c-8f87-4844-9d80-be40651c183f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm =  confusion_matrix(y_true_list, y_pred_list)\n",
    "\n",
    "plot_confusion_matrix(cm = cm, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['pneumonia','normal'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(base_model.state_dict(), './best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyze here how the model works form perspective of good explanations (where model and label agree) and where it does not "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# function to enable displaying matplotlib Figures in notebooks\n",
    "def show_figure(fig): \n",
    "    dummy = plt.figure()\n",
    "    new_manager = dummy.canvas.manager\n",
    "    new_manager.canvas.figure = fig\n",
    "    new_manager.set_window_title(\"Test\")\n",
    "    fig.set_canvas(new_manager.canvas)\n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foxai imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from foxai.context_manager import FoXaiExplainer, ExplainerWithParams, CVClassificationExplainers\n",
    "from foxai.visualizer import mean_channels_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaner configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to focus on GradCam results here as to our knowledge this method gives the most stable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"pneumonia\", \"normal\"]\n",
    "layer = [module for module in model.modules() if isinstance(module, torch.nn.Conv2d)][-1]\n",
    "explainer_list = [\n",
    "    ExplainerWithParams(explainer_name=CVClassificationExplainers.CV_LAYER_GRADCAM_EXPLAINER, layer=layer),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Check where model agrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counter = 0\n",
    "max_samples_explained: int = 40\n",
    "\n",
    "# iterate over dataloader\n",
    "for sample_batch in testloader:\n",
    "    sample_list, label_list = sample_batch\n",
    "    # iterate over all samples in batch\n",
    "    for sample, label in zip(sample_list, label_list):\n",
    "        # add batch size dimension to the data sample\n",
    "        input_data = sample.reshape(1, sample.shape[0], sample.shape[1], sample.shape[2]).to(device)\n",
    "        category_name = categories[label.item()]\n",
    "        with FoXaiExplainer(\n",
    "            model=model,\n",
    "            explainers=[ExplainerWithParams(explainer_name=CVClassificationExplainers.CV_LAYER_GRADCAM_EXPLAINER, layer=layer),],\n",
    "            target=label,\n",
    "        ) as xai_model:\n",
    "            # calculate attributes for every explainer\n",
    "            probs, attributes_dict = xai_model(input_data)\n",
    "        if categories[torch.argmax(_).detach().cpu()] == category_name:\n",
    "            for key, value in attributes_dict.items():\n",
    "\n",
    "                # create figure from attributes and original image           \n",
    "                figure = mean_channels_visualization(attributions=value[0], transformed_img=sample, title= f\"Mean of channels ({key})\")\n",
    "                # save figure to artifact directory\n",
    "                title = categories[torch.argmax(_).detach().cpu()] + \", (\" + category_name +\")\"\n",
    "                imshow(sample, (8,8), title=title)\n",
    "                show_figure(figure)\n",
    "\n",
    "            sample_counter += 1\n",
    "        # if we processed desired number of samples break the loop\n",
    "        if sample_counter > max_samples_explained:\n",
    "            break\n",
    "\n",
    "    # if we processed desired number of samples break the loop\n",
    "    if sample_counter > max_samples_explained:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Check where model disagrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counter = 0\n",
    "max_samples_explained: int = 50\n",
    "\n",
    "# iterate over dataloader\n",
    "for sample_batch in testloader:\n",
    "    sample_list, label_list = sample_batch\n",
    "    # iterate over all samples in batch\n",
    "    for sample, label in zip(sample_list, label_list):\n",
    "        # add batch size dimension to the data sample\n",
    "        input_data = sample.reshape(1, sample.shape[0], sample.shape[1], sample.shape[2]).to(device)\n",
    "        category_name = categories[label.item()]\n",
    "        with FoXaiExplainer(\n",
    "            model=model,\n",
    "            explainers=explainer_list,\n",
    "            target=label,\n",
    "        ) as xai_model:\n",
    "            # calculate attributes for every explainer\n",
    "            probs, attributes_dict = xai_model(input_data)\n",
    "        if categories[torch.argmax(_).detach().cpu()] != category_name:\n",
    "            for key, value in attributes_dict.items():\n",
    "\n",
    "                # create figure from attributes and original image           \n",
    "                figure = mean_channels_visualization(attributions=value[0], transformed_img=sample, title= f\"Mean of channels ({key})\")\n",
    "                # save figure to artifact directory\n",
    "                title = categories[torch.argmax(_).detach().cpu()] + \", (\" + category_name +\")\"\n",
    "                imshow(sample, (8,8), title=title)\n",
    "                show_figure(figure)\n",
    "\n",
    "            sample_counter += 1\n",
    "        # if we processed desired number of samples break the loop\n",
    "        if sample_counter > max_samples_explained:\n",
    "            break\n",
    "\n",
    "    # if we processed desired number of samples break the loop\n",
    "    if sample_counter > max_samples_explained:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations \n",
    "You have successfully trained Pneumonia detecting model, and perform some analysis o the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
