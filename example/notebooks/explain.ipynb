{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, ImageNet\n",
    "\n",
    "from autoxai.explainer.base_explainer import CVExplainer\n",
    "from autoxai.context_manager import AutoXaiExplainer, ExplainerWithParams, Explainers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install missing libraries that are not part of `autoxai` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (1.10.0)\n",
      "Requirement already satisfied: opencv-python in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (4.7.0.68)\n",
      "Requirement already satisfied: seaborn in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (0.12.2)\n",
      "Requirement already satisfied: pyyaml in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (6.0)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (from scipy) (1.23.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (from seaborn) (3.6.2)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# install missing libraries\n",
    "!pip install scipy opencv-python seaborn pyyaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure `CUDA_LAUNCH_BLOCKING=1` to prevent issues with `CUDA` while running GPU-accelerated computations in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for running GPU accelerated computations in notebook requires blocking CUDA launch\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download `YOLOv5` and `ImageNet.yaml` files from https://github.com/ultralytics/yolov5 if not present in local storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if YOLOv5 model and ImageNet.yaml files are present at local storage and if they are not download them\n",
    "![ ! -f \"yolov5s-cls.pt\" ] && wget https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s-cls.pt\n",
    "![ ! -f \"ImageNet.yaml\" ] && wget https://raw.githubusercontent.com/ultralytics/yolov5/master/data/ImageNet.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function that will load model, list of labels and transformation function of a desired model. Currently we support, in this notebook, only a few models: `VGG11`, `ResNet50`, `ViT`, `MobileNetV3` and `YOLOv5`. You can easilly add new models from `torchvision` model zoo and even define Your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from torchvision.transforms._presets import ImageClassification\n",
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    model_name: str,\n",
    ") -> Tuple[torch.nn.Module, List[str], ImageClassification]:\n",
    "    \"\"\"Load model, label list and transformation function used in data preprocessing.\n",
    "\n",
    "    Args:\n",
    "        model_name: Model name. Recognized models are: `vgg11`, `resent50`, `yolov5`,\n",
    "            `vit` and `mobilenetv3`.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: raised if provided model name that is not supported.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of model, list of labels and transformation function.\n",
    "    \"\"\"\n",
    "    # normalize model name to match recognized models\n",
    "    model_name_normalized: str = model_name.lower().strip()\n",
    "    if model_name_normalized == \"yolov5\":\n",
    "        # load YOLOv5 from torch Hub according to https://github.com/ultralytics/yolov5\n",
    "        model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s-cls.pt')\n",
    "\n",
    "        # apply transformations just like in MobileNetV3\n",
    "        transform = torchvision.models.MobileNet_V3_Small_Weights.IMAGENET1K_V1.transforms()\n",
    "        \n",
    "        # load YOLOv5 configuration\n",
    "        with open(\"ImageNet.yaml\") as file:\n",
    "            data = yaml.load(file, Loader=SafeLoader)\n",
    "\n",
    "        # and get only class names\n",
    "        categories = list(data[\"names\"].values())\n",
    "    elif model_name_normalized == \"vgg11\":\n",
    "        weights = torchvision.models.VGG11_Weights.IMAGENET1K_V1\n",
    "\n",
    "        # load model from torchvision model zoo\n",
    "        model = torchvision.models.vgg11(weights=weights)\n",
    "\n",
    "        # get class names\n",
    "        categories = weights.meta[\"categories\"]\n",
    "        transform = weights.transforms()\n",
    "    elif model_name_normalized == \"vit\":\n",
    "        weights = torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "\n",
    "        # load model from torchvision model zoo\n",
    "        model = torchvision.models.vit_b_16(weights=weights)\n",
    "\n",
    "        # get class names\n",
    "        categories = weights.meta[\"categories\"]\n",
    "        transform = weights.transforms()\n",
    "    elif model_name_normalized == \"resnet50\":\n",
    "        weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V1\n",
    "\n",
    "        # load model from torchvision model zoo\n",
    "        model = torchvision.models.resnet50(weights=weights)\n",
    "\n",
    "        # get class names\n",
    "        categories = weights.meta[\"categories\"]\n",
    "        transform = weights.transforms()\n",
    "    elif model_name_normalized == \"mobilenetv3\":\n",
    "        weights = torchvision.models.MobileNet_V3_Small_Weights.IMAGENET1K_V1\n",
    "\n",
    "        # load model from torchvision model zoo\n",
    "        model = torchvision.models.mobilenet_v3_small(weights=weights)\n",
    "\n",
    "        # get class names\n",
    "        categories = weights.meta[\"categories\"]\n",
    "        transform = weights.transforms()\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized model name: {model_name}\")\n",
    "\n",
    "    return model, categories, transform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell below contains configuration of this notebook. We have defined max number of samples to be saved in artifact directory, path to `ImageNet-Mini` dataset downloaded from [Kaggle](https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000), name of the model, batch_size and device to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 1\n",
    "max_samples_explained: int = 5\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_name: str = \"resnet50\"\n",
    "\n",
    "# define directory where explanation artifacts will be stored\n",
    "artifact_dir: str = f\"artifacts/{model_name}/\"\n",
    "\n",
    "# `data_dir` variable contains path to dataset downloaded from https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000.\n",
    "# You have to register in Kaggle to be able to download this dataset.\n",
    "data_dir: str = \"/home/user/Downloads/imagenet-mini\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load specified model, put it in evaluation mode, place it on specified device, download and preprocess `ImageNet-Mini` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model, classes and transformation function\n",
    "model, categories, transform = load_model(model_name=model_name)\n",
    "\n",
    "# put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# place model on specified device (CPU or GPU)\n",
    "model.to(device)\n",
    "\n",
    "# load test dataset - ImageNet-Mini downloaded from Kaggle: https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000\n",
    "imagenet_val = torchvision.datasets.ImageFolder(root=f\"{data_dir}/val\", transform=transform)\n",
    "val_dataloader = DataLoader(imagenet_val, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define list of explainers from `autoxai` package You want to use on specified model. Full list of supported explainers can be found at definition of `Explainers` enum class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of explainers we want to use\n",
    "# full list of supported explainers is present in `Explainers` enum class.\n",
    "explainer_list = [\n",
    "    ExplainerWithParams(explainer_name=Explainers.CV_GRADIENT_SHAP_EXPLAINER),\n",
    "    ExplainerWithParams(explainer_name=Explainers.CV_DECONVOLUTION_EXPLAINER),\n",
    "    ExplainerWithParams(explainer_name=Explainers.CV_INPUT_X_GRADIENT_EXPLAINER),\n",
    "    ExplainerWithParams(explainer_name=Explainers.CV_INTEGRATED_GRADIENTS_EXPLAINER),\n",
    "    ExplainerWithParams(explainer_name=Explainers.CV_OCCLUSION_EXPLAINER),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over dataset and explain predictions given by selected model using all specified explainers. It could take a long time, depending on number of selected explainers and number of samples to explain. During this process new artifacts will be saved in artifact directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/praca/AutoXAI/.venv/lib/python3.8/site-packages/captum/attr/_core/guided_backprop_deconvnet.py:64: UserWarning: Setting backward hooks on ReLU activations.The hooks will be removed after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m\n\u001b[1;32m     15\u001b[0m input_data \u001b[39m=\u001b[39m input_data\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m AutoXaiExplainer(\n\u001b[1;32m     17\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     18\u001b[0m     explainers\u001b[39m=\u001b[39mexplainer_list,\n\u001b[1;32m     19\u001b[0m     target\u001b[39m=\u001b[39mlabel,\n\u001b[1;32m     20\u001b[0m ) \u001b[39mas\u001b[39;00m xai_model:\n\u001b[1;32m     21\u001b[0m     \u001b[39m# calculate attributes for every explainer\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     _, attributes_dict \u001b[39m=\u001b[39m xai_model(input_data)\n\u001b[1;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m attributes_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     25\u001b[0m     \u001b[39m# create directory for every explainer artifacts\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     artifact_explainer_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(artifact_dir, key)\n",
      "File \u001b[0;32m~/praca/AutoXAI/autoxai/context_manager.py:240\u001b[0m, in \u001b[0;36mAutoXaiExplainer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[39m# run explainer\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     explainer_kwargs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplainer_map[explainer_name]\u001b[39m.\u001b[39mkwargs\n\u001b[1;32m    239\u001b[0m     explanations[explainer_name] \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 240\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplainer_map[explainer_name]\n\u001b[1;32m    241\u001b[0m         \u001b[39m.\u001b[39;49mexplainer_class\u001b[39m.\u001b[39;49mcalculate_features(\n\u001b[1;32m    242\u001b[0m             model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    243\u001b[0m             input_data\u001b[39m=\u001b[39;49minput_tensor,\n\u001b[1;32m    244\u001b[0m             pred_label_idx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget,\n\u001b[1;32m    245\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mexplainer_kwargs,\n\u001b[1;32m    246\u001b[0m         )\n\u001b[1;32m    247\u001b[0m         \u001b[39m.\u001b[39mdetach()\n\u001b[1;32m    248\u001b[0m         \u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    249\u001b[0m     )\n\u001b[1;32m    250\u001b[0m     input_tensor\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[39m# restore tensor requires grad state\u001b[39;00m\n",
      "File \u001b[0;32m~/praca/AutoXAI/autoxai/explainer/occulusion.py:36\u001b[0m, in \u001b[0;36mOcculusionCVExplainer.calculate_features\u001b[0;34m(self, model, input_data, pred_label_idx, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m sliding_window_shapes \u001b[39m=\u001b[39m (input_data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], window_value, window_value)\n\u001b[1;32m     34\u001b[0m occlusion \u001b[39m=\u001b[39m Occlusion(model)\n\u001b[0;32m---> 36\u001b[0m attributions \u001b[39m=\u001b[39m occlusion\u001b[39m.\u001b[39;49mattribute(\n\u001b[1;32m     37\u001b[0m     input_data,\n\u001b[1;32m     38\u001b[0m     strides\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m     39\u001b[0m     target\u001b[39m=\u001b[39;49mpred_label_idx,\n\u001b[1;32m     40\u001b[0m     sliding_window_shapes\u001b[39m=\u001b[39;49msliding_window_shapes,\n\u001b[1;32m     41\u001b[0m     baselines\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m attributions\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/captum/log/__init__.py:35\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/captum/attr/_core/occlusion.py:256\u001b[0m, in \u001b[0;36mOcclusion.attribute\u001b[0;34m(self, inputs, sliding_window_shapes, strides, baselines, target, additional_forward_args, perturbations_per_eval, show_progress)\u001b[0m\n\u001b[1;32m    249\u001b[0m     shift_counts\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    250\u001b[0m         \u001b[39mtuple\u001b[39m(\n\u001b[1;32m    251\u001b[0m             np\u001b[39m.\u001b[39madd(np\u001b[39m.\u001b[39mceil(np\u001b[39m.\u001b[39mdivide(current_shape, strides[i]))\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m), \u001b[39m1\u001b[39m)\n\u001b[1;32m    252\u001b[0m         )\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    255\u001b[0m \u001b[39m# Use ablation attribute method\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mattribute\u001b[39m.\u001b[39;49m__wrapped__(\n\u001b[1;32m    257\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    258\u001b[0m     inputs,\n\u001b[1;32m    259\u001b[0m     baselines\u001b[39m=\u001b[39;49mbaselines,\n\u001b[1;32m    260\u001b[0m     target\u001b[39m=\u001b[39;49mtarget,\n\u001b[1;32m    261\u001b[0m     additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[1;32m    262\u001b[0m     perturbations_per_eval\u001b[39m=\u001b[39;49mperturbations_per_eval,\n\u001b[1;32m    263\u001b[0m     sliding_window_tensors\u001b[39m=\u001b[39;49msliding_window_tensors,\n\u001b[1;32m    264\u001b[0m     shift_counts\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(shift_counts),\n\u001b[1;32m    265\u001b[0m     strides\u001b[39m=\u001b[39;49mstrides,\n\u001b[1;32m    266\u001b[0m     show_progress\u001b[39m=\u001b[39;49mshow_progress,\n\u001b[1;32m    267\u001b[0m )\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/captum/attr/_core/feature_ablation.py:354\u001b[0m, in \u001b[0;36mFeatureAblation.attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, feature_mask, perturbations_per_eval, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39mfor\u001b[39;00m (\n\u001b[1;32m    338\u001b[0m     current_inputs,\n\u001b[1;32m    339\u001b[0m     current_add_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[39m# modified_eval dimensions: 1D tensor with length\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     \u001b[39m# equal to #num_examples * #features in batch\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     modified_eval \u001b[39m=\u001b[39m _run_forward(\n\u001b[1;32m    355\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_func,\n\u001b[1;32m    356\u001b[0m         current_inputs,\n\u001b[1;32m    357\u001b[0m         current_target,\n\u001b[1;32m    358\u001b[0m         current_add_args,\n\u001b[1;32m    359\u001b[0m     )\n\u001b[1;32m    361\u001b[0m     \u001b[39mif\u001b[39;00m show_progress:\n\u001b[1;32m    362\u001b[0m         attr_progress\u001b[39m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/captum/_utils/common.py:456\u001b[0m, in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    453\u001b[0m inputs \u001b[39m=\u001b[39m _format_input(inputs)\n\u001b[1;32m    454\u001b[0m additional_forward_args \u001b[39m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[0;32m--> 456\u001b[0m output \u001b[39m=\u001b[39m forward_func(\n\u001b[1;32m    457\u001b[0m     \u001b[39m*\u001b[39;49m(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49madditional_forward_args)\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;49;00m additional_forward_args \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    459\u001b[0m     \u001b[39melse\u001b[39;49;00m inputs\n\u001b[1;32m    460\u001b[0m )\n\u001b[1;32m    461\u001b[0m \u001b[39mreturn\u001b[39;00m _select_targets(output, target)\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/torchvision/models/resnet.py:276\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[0;32m--> 276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[1;32m    279\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/torchvision/models/resnet.py:152\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out)\n\u001b[1;32m    151\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(out)\n\u001b[0;32m--> 152\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu(out)\n\u001b[1;32m    154\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(out)\n\u001b[1;32m    155\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(out)\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/praca/AutoXAI/.venv/lib/python3.8/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample: torch.Tensor\n",
    "label: int\n",
    "\n",
    "sample_counter: int = 0\n",
    "\n",
    "# iterate over dataloader\n",
    "for sample_batch in val_dataloader:\n",
    "    sample_list, label_list = sample_batch\n",
    "    # iterate over all samples in batch\n",
    "    for sample, label in zip(sample_list, label_list):\n",
    "        # add batch size dimension to the data sample\n",
    "        input_data = sample.reshape(1, sample.shape[0], sample.shape[1], sample.shape[2]).to(device)\n",
    "        \n",
    "        # move it to specified device\n",
    "        input_data = input_data.to(device)\n",
    "        with AutoXaiExplainer(\n",
    "            model=model,\n",
    "            explainers=explainer_list,\n",
    "            target=label,\n",
    "        ) as xai_model:\n",
    "            # calculate attributes for every explainer\n",
    "            _, attributes_dict = xai_model(input_data)\n",
    "\n",
    "        for key, value in attributes_dict.items():\n",
    "            # create directory for every explainer artifacts\n",
    "            artifact_explainer_dir = os.path.join(artifact_dir, key)\n",
    "            if not os.path.exists(artifact_explainer_dir):\n",
    "                os.makedirs(artifact_explainer_dir)\n",
    "\n",
    "            # create figure from attributes and original image\n",
    "            figure = CVExplainer.visualize(attributions=value, transformed_img=sample)\n",
    "\n",
    "            # save figure to artifact directory\n",
    "            figure.savefig(os.path.join(artifact_explainer_dir, f\"artifact_{sample_counter}_{categories[label.item()]}.png\"))\n",
    "\n",
    "        sample_counter += 1\n",
    "        # if we processed desired number of samples break the loop\n",
    "        if sample_counter > max_samples_explained:\n",
    "            break\n",
    "\n",
    "    # if we processed desired number of samples break the loop\n",
    "    if sample_counter > max_samples_explained:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are algorithms that are computing explanations on the level of single layer. You have to select one layer to explain against it. Many algorithms are using `Conv2d` layers to explain. In the cell below we are fetching last convolutional layer from network to explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = [module for module in model.modules() if isinstance(module, torch.nn.Conv2d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 15:39:24,923 INFO autoxai.explainer.base_explainer - No negative attributes in the explained model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define list of explainers we want to use\n",
    "# full list of supported explainers is present in `Explainers` enum class.\n",
    "explainer_list = [\n",
    "    ExplainerWithParams(explainer_name=Explainers.CV_GUIDEDGRADCAM_EXPLAINER, layer=layer),\n",
    "    ExplainerWithParams(explainer_name=Explainers.CV_LAYER_GRADCAM_EXPLAINER, layer=layer),\n",
    "]\n",
    "sample_counter = 0\n",
    "\n",
    "# iterate over dataloader\n",
    "for sample_batch in val_dataloader:\n",
    "    sample_list, label_list = sample_batch\n",
    "    # iterate over all samples in batch\n",
    "    for sample, label in zip(sample_list, label_list):\n",
    "        # add batch size dimension to the data sample\n",
    "        input_data = sample.reshape(1, sample.shape[0], sample.shape[1], sample.shape[2]).to(device)\n",
    "        \n",
    "        # move it to specified device\n",
    "        input_data = input_data.to(device)\n",
    "        with AutoXaiExplainer(\n",
    "            model=model,\n",
    "            explainers=explainer_list,\n",
    "            target=label,\n",
    "        ) as xai_model:\n",
    "            # calculate attributes for every explainer\n",
    "            _, attributes_dict = xai_model(input_data)\n",
    "\n",
    "        for key, value in attributes_dict.items():\n",
    "            # create directory for every explainer artifacts\n",
    "            artifact_explainer_dir = os.path.join(artifact_dir, key)\n",
    "            if not os.path.exists(artifact_explainer_dir):\n",
    "                os.makedirs(artifact_explainer_dir)\n",
    "\n",
    "            # create figure from attributes and original image\n",
    "            figure = CVExplainer.visualize(attributions=value, transformed_img=sample)\n",
    "\n",
    "            # save figure to artifact directory\n",
    "            figure.savefig(os.path.join(artifact_explainer_dir, f\"artifact_{sample_counter}_{categories[label.item()]}.png\"))\n",
    "\n",
    "        sample_counter += 1\n",
    "        # if we processed desired number of samples break the loop\n",
    "        if sample_counter > max_samples_explained:\n",
    "            break\n",
    "\n",
    "    # if we processed desired number of samples break the loop\n",
    "    if sample_counter > max_samples_explained:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b6969b3572db2a15157531a0a5e0349bcc012e2552026eae9f4d51dce8b0863"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
