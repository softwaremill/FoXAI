{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook for XAI algorithms quantitive evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is based on [Quantus notebook](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_ImageNet_Quantification_with_Quantus.ipynb) with guideline how to get started using [Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus) library.\n",
    "\n",
    "In this tutorial, You will see how You can use explainable algorithms to study pre-trained model decision and quantify those algorithms using metrics provided by Quantus library. We will take a pre-trained model, sample images, run several explainable methods and quantify them for further analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install missing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python seaborn quantus cachetools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "from foxai.context_manager import FoXaiExplainer, ExplainerWithParams, CVClassificationExplainers\n",
    "from foxai.visualizer import mean_channels_visualization\n",
    "\n",
    "import quantus\n",
    "import numpy as np\n",
    "from foxai.explainer.computer_vision.algorithm.integrated_gradients import IntegratedGradientsCVExplainer\n",
    "from foxai.explainer.computer_vision.algorithm.input_x_gradient import InputXGradientCVExplainer\n",
    "from foxai.explainer.computer_vision.algorithm.gradient_shap import GradientSHAPCVExplainer\n",
    "from foxai.explainer.computer_vision.algorithm.guided_backprop import GuidedBackpropCVExplainer\n",
    "from foxai.explainer.computer_vision.algorithm.gradcam import LayerGradCAMCVExplainer\n",
    "from foxai.explainer.computer_vision.algorithm.gradcam import GuidedGradCAMCVExplainer\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.transforms._presets import ImageClassification\n",
    "from typing import Tuple, List, Dict, Callable, Optional\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "# Plotting specifics.\n",
    "from matplotlib.patches import Circle, RegularPolygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from matplotlib.projections import register_projection\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure `CUDA_LAUNCH_BLOCKING=1` to prevent issues with `CUDA` while running GPU-accelerated computations in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define custom functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function that will load model, list of labels and transformation function of a desired model. Currently we support, in this notebook, only a few models: `VGG11`, `ResNet50`, `ViT`, `MobileNetV3` and `YOLOv5`. You can easilly add new models from `torchvision` model zoo and even define Your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    model_name: str,\n",
    ") -> Tuple[torch.nn.Module, List[str], ImageClassification]:\n",
    "    \"\"\"Load model, label list and transformation function used in data preprocessing.\n",
    "\n",
    "    Args:\n",
    "        model_name: Model name. Recognized models are: `vgg11`, `resent50`, `vit` and\n",
    "            `mobilenetv3`.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: raised if provided model name that is not supported.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of model, list of labels and transformation function.\n",
    "    \"\"\"\n",
    "    # normalize model name to match recognized models\n",
    "    model_name_normalized: str = model_name.lower().strip()\n",
    "    if model_name_normalized == \"vgg11\":\n",
    "        weights = torchvision.models.VGG11_Weights.IMAGENET1K_V1\n",
    "\n",
    "        # load model from torchvision model zoo\n",
    "        model = torchvision.models.vgg11(weights=weights)\n",
    "\n",
    "        # get class names\n",
    "        categories = weights.meta[\"categories\"]\n",
    "        transform = weights.transforms()\n",
    "    elif model_name_normalized == \"vit\":\n",
    "        weights = torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "\n",
    "        # load model from torchvision model zoo\n",
    "        model = torchvision.models.vit_b_16(weights=weights)\n",
    "\n",
    "        # get class names\n",
    "        categories = weights.meta[\"categories\"]\n",
    "        transform = weights.transforms()\n",
    "    elif model_name_normalized == \"resnet50\":\n",
    "        weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V1\n",
    "\n",
    "        # load model from torchvision model zoo\n",
    "        model = torchvision.models.resnet50(weights=weights)\n",
    "\n",
    "        # get class names\n",
    "        categories = weights.meta[\"categories\"]\n",
    "        transform = weights.transforms()\n",
    "    elif model_name_normalized == \"mobilenetv3\":\n",
    "        weights = torchvision.models.MobileNet_V3_Small_Weights.IMAGENET1K_V1\n",
    "\n",
    "        # load model from torchvision model zoo\n",
    "        model = torchvision.models.mobilenet_v3_small(weights=weights)\n",
    "\n",
    "        # get class names\n",
    "        categories = weights.meta[\"categories\"]\n",
    "        transform = weights.transforms()\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized model name: {model_name}\")\n",
    "\n",
    "    return model, categories, transform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell below contains configuration of this notebook. We have defined max number of samples to be saved in artifact directory, path to `ImageNet-Mini` dataset downloaded from [Kaggle](https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000), name of the model, batch_size and device to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 2\n",
    "max_samples_explained: int = 10\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_name: str = \"mobilenetv3\"\n",
    "\n",
    "# define directory where explanation artifacts will be stored\n",
    "artifact_dir: str = f\"artifacts/{model_name}/\"\n",
    "\n",
    "# `data_dir` variable contains path to dataset downloaded from https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000.\n",
    "# You have to register in Kaggle to be able to download this dataset.\n",
    "data_dir: str = \"/home/user/Downloads/imagenet-mini\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load specified model, put it in evaluation mode, place it on specified device, download and preprocess `ImageNet-Mini` dataset. Trasformation function is used to match training dataset preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model, classes and transformation function\n",
    "model, categories, transform = load_model(model_name=model_name)\n",
    "\n",
    "# put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# place model on specified device (CPU or GPU)\n",
    "model.to(device)\n",
    "\n",
    "# get last layer to explain\n",
    "layer = [module for module in model.modules() if isinstance(module, torch.nn.Conv2d)][-1]\n",
    "\n",
    "# load test dataset - ImageNet-Mini downloaded from Kaggle: https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000\n",
    "imagenet_val = torchvision.datasets.ImageFolder(root=f\"{data_dir}/val\", transform=transform)\n",
    "val_dataloader = DataLoader(imagenet_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct notebook to display figures inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source code: https://matplotlib.org/stable/gallery/specialty_plots/radar_chart.html.\n",
    "\n",
    "def radar_factory(num_vars, frame='circle'):\n",
    "    \"\"\"Create a radar chart with `num_vars` axes.\n",
    "\n",
    "    This function creates a RadarAxes projection and registers it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_vars : int\n",
    "        Number of variables for radar chart.\n",
    "    frame : {'circle' | 'polygon'}\n",
    "        Shape of frame surrounding axes.\n",
    "    \"\"\"\n",
    "    # calculate evenly-spaced axis angles\n",
    "    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
    "\n",
    "    class RadarAxes(PolarAxes):\n",
    "\n",
    "        name = 'radar'\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            # rotate plot such that the first axis is at the top\n",
    "            self.set_theta_zero_location('N')\n",
    "\n",
    "        def fill(self, *args, closed=True, **kwargs):\n",
    "            \"\"\"Override fill so that line is closed by default.\"\"\"\n",
    "            return super().fill(closed=closed, *args, **kwargs)\n",
    "\n",
    "        def plot(self, *args, **kwargs):\n",
    "            \"\"\"Override plot so that line is closed by default.\"\"\"\n",
    "            lines = super().plot(*args, **kwargs)\n",
    "            for line in lines:\n",
    "                self._close_line(line)\n",
    "\n",
    "        def _close_line(self, line):\n",
    "            x, y = line.get_data()\n",
    "            # FIXME: markers at x[0], y[0] get doubled-up\n",
    "            if x[0] != x[-1]:\n",
    "                x = np.concatenate((x, [x[0]]))\n",
    "                y = np.concatenate((y, [y[0]]))\n",
    "                line.set_data(x, y)\n",
    "\n",
    "        def set_varlabels(self, labels, angles=None):\n",
    "            self.set_thetagrids(angles=np.degrees(theta), labels=labels)\n",
    "\n",
    "        def _gen_axes_patch(self):\n",
    "            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n",
    "            # in axes coordinates.\n",
    "            if frame == 'circle':\n",
    "                return Circle((0.5, 0.5), 0.5)\n",
    "            elif frame == 'polygon':\n",
    "                return RegularPolygon((0.5, 0.5), num_vars,\n",
    "                                      radius=.5, edgecolor=\"k\")\n",
    "            else:\n",
    "                raise ValueError(\"unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "        def draw(self, renderer):\n",
    "            \"\"\" Draw. If frame is polygon, make gridlines polygon-shaped.\"\"\"\n",
    "            if frame == 'polygon':\n",
    "                gridlines = self.yaxis.get_gridlines()\n",
    "                for gl in gridlines:\n",
    "                    gl.get_path()._interpolation_steps = num_vars\n",
    "            super().draw(renderer)\n",
    "\n",
    "\n",
    "        def _gen_axes_spines(self):\n",
    "            if frame == 'circle':\n",
    "                return super()._gen_axes_spines()\n",
    "            elif frame == 'polygon':\n",
    "                # spine_type must be 'left'/'right'/'top'/'bottom'/'circle'.\n",
    "                spine = Spine(axes=self,\n",
    "                              spine_type='circle',\n",
    "                              path=Path.unit_regular_polygon(num_vars))\n",
    "                # unit_regular_polygon gives a polygon of radius 1 centered at\n",
    "                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n",
    "                # 0.5) in axes coordinates.\n",
    "                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n",
    "                                    + self.transAxes)\n",
    "\n",
    "                return {'polar': spine}\n",
    "            else:\n",
    "                raise ValueError(\"unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "    register_projection(RadarAxes)\n",
    "    return theta\n",
    "\n",
    "\n",
    "# Plotting configs.\n",
    "sns.set(font_scale=1.8)\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams['ytick.labelleft'] = True\n",
    "plt.rcParams['xtick.labelbottom'] = True\n",
    "\n",
    "def plot_spyder_graph(\n",
    "    df_normalised_rank: pd.DataFrame,\n",
    "    xai_methods: List[str],\n",
    "    metric_weight_dict: Dict[str, float],\n",
    "    include_titles: bool = True,\n",
    "    include_legend: bool = True,\n",
    ") -> None:\n",
    "    # Make spyder graph!\n",
    "    data = [df_normalised_rank.columns.values, (df_normalised_rank.to_numpy())]\n",
    "    theta = radar_factory(len(data[0]), frame='polygon')\n",
    "    _ = data.pop(0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(11, 11), subplot_kw=dict(projection='radar'))\n",
    "    fig.subplots_adjust(top=0.85, bottom=0.05)\n",
    "    for d, method in zip(data[0], xai_methods):\n",
    "        _ = ax.plot(\n",
    "            theta,\n",
    "            d,\n",
    "            label=method,\n",
    "            linewidth=5.0\n",
    "        )\n",
    "        ax.fill(theta, d, alpha=0.15)\n",
    "\n",
    "    # Set lables.\n",
    "    if include_titles:\n",
    "        ax.set_varlabels(labels=list(metric_weight_dict.keys()))\n",
    "    else:\n",
    "        ax.set_varlabels(labels=[]) \n",
    "\n",
    "    # Set a title.\n",
    "    ax.set_title(\"Summary of Explainer Quantification\",  position=(0.5, 1.1), ha='center', fontsize=15)\n",
    "\n",
    "    # Put a legend to the right of the current axis.\n",
    "    if include_legend:\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foxai_explainer_wrapper(\n",
    "    explainer, model, inputs, targets, *args, **kwargs\n",
    ") -> np.array:\n",
    "    \"\"\"Wrapper aorund captum's Saliency implementation.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Set model in evaluate mode.\n",
    "    model.to(kwargs.get(\"device\", None))\n",
    "    model.eval()\n",
    "\n",
    "    if not isinstance(inputs, torch.Tensor):\n",
    "        inputs = torch.as_tensor(inputs)\n",
    "\n",
    "    if not isinstance(targets, torch.Tensor):\n",
    "        targets = (\n",
    "            torch.as_tensor(targets).long()\n",
    "        )  # inputs = inputs.reshape(-1, 3, 224, 224)\n",
    "\n",
    "    assert (\n",
    "        len(np.shape(inputs)) == 4\n",
    "    ), \"Inputs should be shaped (nr_samples, nr_channels, img_size, img_size) e.g., (1, 3, 224, 224).\"\n",
    "\n",
    "    explanation = (\n",
    "        explainer()\n",
    "        .calculate_features(model, inputs, targets)\n",
    "        .sum(axis=1)\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if not kwargs.get(\"normalise\", None):\n",
    "        explanation = quantus.normalise_func.normalise_by_negative(explanation)\n",
    "\n",
    "    if isinstance(explanation, torch.Tensor):\n",
    "        if explanation.requires_grad:\n",
    "            return explanation.detach().cpu().detach().numpy()\n",
    "        return explanation.detach().cpu().numpy()\n",
    "\n",
    "    return explanation\n",
    "\n",
    "integrated_gradients_explainer = partial(foxai_explainer_wrapper, explainer=IntegratedGradientsCVExplainer)\n",
    "input_x_gradients_explainer = partial(foxai_explainer_wrapper, explainer=InputXGradientCVExplainer)\n",
    "gradient_shap_explainer = partial(foxai_explainer_wrapper, explainer=GradientSHAPCVExplainer)\n",
    "guided_backprop_explainer = partial(foxai_explainer_wrapper, explainer=GuidedBackpropCVExplainer)\n",
    "layer_gradcam_explainer = partial(foxai_explainer_wrapper, explainer=LayerGradCAMCVExplainer)\n",
    "guided_gradcam_explainer = partial(foxai_explainer_wrapper, explainer=GuidedGradCAMCVExplainer)\n",
    "\n",
    "explainer_wrapper = {\n",
    "    \"IntegratedGradients\": integrated_gradients_explainer,\n",
    "    \"InputXGradient\": input_x_gradients_explainer,\n",
    "    \"GradientSHAP\": gradient_shap_explainer,\n",
    "    \"GuidedBackpropagation\": guided_backprop_explainer,\n",
    "    \"LayerGradCAM\": layer_gradcam_explainer,\n",
    "    \"GuidedGradCAM\": guided_gradcam_explainer,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    max_batch_no: int,\n",
    ") -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "    \"\"\"Get list of batches of data sample and labels.\n",
    "\n",
    "    Args:\n",
    "        dataloader: Dataloader to iterate.\n",
    "        device: torch.Device to place batches to.\n",
    "        max_batch_no: Number of batches.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists of batches of data sample and labels.\n",
    "    \"\"\"\n",
    "    x_batch_list: List[torch.Tensor] = []\n",
    "    y_batch_list: List[torch.Tensor] = []\n",
    "    for index, batch in enumerate(dataloader):\n",
    "        sample_batch, label_batch = batch\n",
    "        sample_batch.to(device)\n",
    "        label_batch.to(device)\n",
    "\n",
    "        x_batch_list.append(sample_batch)\n",
    "        y_batch_list.append(label_batch)\n",
    "\n",
    "        if index > max_batch_no:\n",
    "            break\n",
    "\n",
    "    return x_batch_list, y_batch_list\n",
    "\n",
    "def compute_metrics_for_explainers(\n",
    "    x_batch_list: List[torch.Tensor],\n",
    "    y_batch_list: List[torch.Tensor],\n",
    "    explainer_wrapper: Dict[str, Callable],\n",
    "    model: torch.nn.Module,\n",
    "    layer: torch.nn.Module,\n",
    "    device: torch.device,\n",
    ") -> Dict[str, Dict[str, List[np.ndarray]]]:\n",
    "    explainer_attributes: Dict[str, Dict[str, List[np.ndarray]]] = {key: [] for key in explainer_wrapper.keys()}\n",
    "    for sample_batch, label_batch in zip(x_batch_list, y_batch_list):\n",
    "        for explainer_name, explainer in explainer_wrapper.items():\n",
    "            start = time()\n",
    "            \n",
    "            attributes = explainer(\n",
    "                model=model,\n",
    "                inputs=sample_batch,\n",
    "                targets=label_batch,\n",
    "                **{\n",
    "                    \"device\": device,\n",
    "                    \"layer\": layer,    \n",
    "                },\n",
    "            )\n",
    "            explainer_attributes[explainer_name].append(attributes)\n",
    "\n",
    "            stop = time()\n",
    "            print(f\"Explainer name: {explainer_name}, time: {round(stop - start, 2)} seconds\")\n",
    "\n",
    "\n",
    "    for explainer_key in explainer_attributes.keys():\n",
    "        explainer_attributes[explainer_key] = np.vstack(explainer_attributes[explainer_key])\n",
    "\n",
    "    return explainer_attributes\n",
    "\n",
    "x_batch_list, y_batch_list = get_batch_data(\n",
    "    dataloader=val_dataloader,\n",
    "    device=device,\n",
    "    max_batch_no=2,\n",
    ")\n",
    "x_batch = np.vstack(x_batch_list)\n",
    "y_batch = np.hstack(y_batch_list)\n",
    "explainer_attributes = compute_metrics_for_explainers(\n",
    "    x_batch_list=x_batch_list,\n",
    "    y_batch_list=y_batch_list,\n",
    "    explainer_wrapper=explainer_wrapper,\n",
    "    model=model,\n",
    "    layer=layer,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attributes(\n",
    "    index: int,\n",
    "    explanations: Dict[str, List[np.ndarray]],\n",
    "    max_height: int = 224,\n",
    "    max_width: int = 224,\n",
    ") -> None:\n",
    "    \"\"\"Visualize attributes from set of explainers in one plot.\n",
    "\n",
    "    Args:\n",
    "        index: Index of sample to visualize.\n",
    "        explanations: Dictionary of attributes.\n",
    "        max_height: Maximum height of image. Defaults to 224.\n",
    "        max_width: Maximum width of image. Defaults to 224.\n",
    "    \"\"\"\n",
    "    _, axes = plt.subplots(nrows=1, ncols=1+len(explanations), figsize=(15, 8))\n",
    "    axes[0].imshow(\n",
    "        np.moveaxis(\n",
    "            quantus.normalise_func.denormalise(\n",
    "                x_batch[index],\n",
    "                mean=np.array([0.485, 0.456, 0.406]),\n",
    "                std=np.array([0.229, 0.224, 0.225])\n",
    "            ),\n",
    "            0,\n",
    "            -1\n",
    "        ),\n",
    "        vmin=0.0,\n",
    "        vmax=1.0\n",
    "    )\n",
    "    axes[0].set_title(f\"ImageNet class {y_batch[index].item()}\", rotation=60)\n",
    "    axes[0].axis(\"off\")\n",
    "    for i, (k, v) in enumerate(explanations.items()):\n",
    "        if explanations[k][index].shape[0] < max_height:\n",
    "            image = cv2.resize(explanations[k][index], None, fx=max_width, fy=max_height, interpolation=cv2.INTER_NEAREST)\n",
    "        else:\n",
    "            image = explanations[k][index]\n",
    "        axes[i+1].imshow(\n",
    "            quantus.normalise_func.normalise_by_negative(image),\n",
    "            cmap=\"seismic\",\n",
    "            vmin=-1.0,\n",
    "            vmax=1.0\n",
    "        )\n",
    "        axes[i+1].set_title(f\"{k}\", rotation=60)\n",
    "        axes[i+1].axis(\"off\")\n",
    "\n",
    "\n",
    "visualize_attributes(index=1, explanations=explainer_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we will define metrics that will be used for quantifying explainable algorithms. Those metrics, according to authors of Quantus library, are sensitive to different configurations and need to be carefully parametrized. To better understand how those parameters influence metric consider reading original papers describing metrics and study implementation code of Quantus library.\n",
    "\n",
    "Library itself provides guidance about different parameters that may influence results in logs. It also provides original publication details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XAI methods and metrics.\n",
    "xai_methods = list(explainer_attributes.keys())\n",
    "metrics = {\n",
    "    \"EffectiveComplexity\": quantus.EffectiveComplexity(\n",
    "        abs=True,\n",
    "        normalise=False,\n",
    "        aggregate_func=np.mean,\n",
    "        return_aggregate=True,\n",
    "        disable_warnings=False,\n",
    "    ),\n",
    "    \"EffectiveComplexityNormalised\": quantus.EffectiveComplexity(\n",
    "        abs=True,\n",
    "        normalise=True,\n",
    "        aggregate_func=np.mean,\n",
    "        return_aggregate=True,\n",
    "        disable_warnings=False,\n",
    "    ),\n",
    "    \"Sparseness\": quantus.Sparseness(\n",
    "        abs=True,\n",
    "        normalise=False,\n",
    "        aggregate_func=np.mean,\n",
    "        return_aggregate=True,\n",
    "        disable_warnings=False,\n",
    "    ),\n",
    "    \"SparsenessNormalised\": quantus.Sparseness(\n",
    "        abs=True,\n",
    "        normalise=True,\n",
    "        aggregate_func=np.mean,\n",
    "        return_aggregate=True,\n",
    "        disable_warnings=False,\n",
    "    ),\n",
    "    \"Complexity\": quantus.Complexity(\n",
    "        abs=True,\n",
    "        normalise=False,\n",
    "    ),\n",
    "    \"ComplexityNormalised\": quantus.Complexity(\n",
    "        abs=True,\n",
    "        normalise=True,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    xai_methods: Dict[str, Callable],\n",
    "    x_batch: torch.Tensor,\n",
    "    y_batch: torch.Tensor,\n",
    "    explainer_attributes: Dict[str, List[np.ndarray]],\n",
    "    metrics: Dict[str, Callable],\n",
    ") -> Dict[str, Dict[str, List[np.ndarray]]]:\n",
    "    \"\"\"Calculate metric score for each explainer.\n",
    "\n",
    "    Args:\n",
    "        xai_methods: Dictionary of explainer name and function.\n",
    "        x_batch: Image batch tensor.\n",
    "        y_batch: Label batch tensor.\n",
    "        explainer_attributes: Dictionary of attributes for each explainer.\n",
    "        metrics: Dictionary of metric name and function.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with name of metric and score for each explainer algorithm.\n",
    "    \"\"\"\n",
    "    # run quantification analysis\n",
    "    results = {method : {} for method in xai_methods}\n",
    "\n",
    "    if isinstance(x_batch, torch.Tensor):\n",
    "        x_batch = x_batch.detach().cpu().numpy()\n",
    "    if isinstance(y_batch, torch.Tensor):\n",
    "        y_batch = y_batch.detach().cpu().numpy()\n",
    "\n",
    "    for method in explainer_attributes.keys():\n",
    "        for metric, metric_func in metrics.items():\n",
    "            start = time()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Get scores and append results.\n",
    "            scores = metric_func(\n",
    "                model=model,\n",
    "                x_batch=x_batch,\n",
    "                y_batch=y_batch,\n",
    "                a_batch=explainer_attributes[method],\n",
    "                s_batch=None,\n",
    "                device=device,\n",
    "                explain_func=explainer_wrapper[method],\n",
    "                explain_func_kwargs={\n",
    "                    \"device\": device,\n",
    "                },\n",
    "            )\n",
    "            results[method][metric] = scores\n",
    "            stop = time()\n",
    "            print(f\"Evaluating {metric} of {method} method, time: {round(stop - start, 2)} seconds\")\n",
    "\n",
    "            # Empty cache.\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return results\n",
    "\n",
    "results = compute_metrics(\n",
    "    xai_methods=xai_methods,\n",
    "    x_batch=x_batch,\n",
    "    y_batch=y_batch,\n",
    "    explainer_attributes=explainer_attributes,\n",
    "    metrics=metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, metric_dict in results.items():\n",
    "    print(f\"Algorithm '{key}'\")\n",
    "    for metric_name, metric_value in metric_dict.items():\n",
    "        print(f\"\\t'{metric_name}': {round(metric_value[0], 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_ranks(\n",
    "        metrics: Dict[str, Callable],\n",
    "        xai_methods: Dict[str, List[np.ndarray]],\n",
    "        results: Dict[str, Dict[str, List[np.ndarray]]],\n",
    "        lower_is_better_metric_names: List[str],\n",
    ") -> pd.DataFrame:\n",
    "    # Postprocessing of scores: to get how the different explanation methods rank across criteria.\n",
    "    results_agg = {}\n",
    "    for method in xai_methods:\n",
    "        results_agg[method] = {}\n",
    "        for metric, _ in metrics.items():\n",
    "            results_agg[method][metric] = np.mean(results[method][metric])\n",
    "\n",
    "    df = pd.DataFrame.from_dict(results_agg)\n",
    "    df = df.T.abs()\n",
    "\n",
    "    # Take inverse ranking for EffectiveComplexity, EffectiveComplexityNormalized, Sparsity and SparsityNormalized since lower is better.\n",
    "    df_normalised = df.loc[:].apply(lambda x: x / x.max())\n",
    "\n",
    "    # apply normalisation for `lower is beter` metrics\n",
    "    for metric_name in lower_is_better_metric_names:\n",
    "        df_normalised[metric_name] = df[metric_name].min() / df[metric_name].values\n",
    "\n",
    "    df_normalised_rank = df_normalised.rank()\n",
    "\n",
    "    # replace NaN with zeros\n",
    "    for column in df.columns:\n",
    "        df_normalised_rank[column] = df_normalised_rank[column].replace(np.nan, -1)\n",
    "\n",
    "    return df_normalised_rank\n",
    "\n",
    "df_normalised_rank = calculate_metric_ranks(\n",
    "    metrics=metrics,\n",
    "    xai_methods=xai_methods,\n",
    "    results=results,\n",
    "    lower_is_better_metric_names=[\n",
    "        \"EffectiveComplexityNormalised\",\n",
    "        \"EffectiveComplexity\",\n",
    "        \"Sparseness\",\n",
    "        \"SparsenessNormalised\",\n",
    "    ],\n",
    ")\n",
    "df_normalised_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate and sort XAI algorithm score based on weighted XAI metrics partial scores. You can use this approach to automatically select the most prominent XAI algorithm for given dataset and model. This approach is only demonstration of possible use case scenario. At the moment we don't have one robust approach to select XAI algorithm without human intervention. When selecting XAI metrics you need to know them in depth to use and interpret them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explainers_score(\n",
    "    df_normalised_rank: pd.DataFrame,\n",
    "    metric_weight_dict: Optional[Dict[str, float]] = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Return dictionary of explainers with aggregated weighted metric score in descending order.\n",
    "\n",
    "    Args:\n",
    "        metric_weight_dict: Dictionary with weights for every metric.\n",
    "        df_normalised_rank: DataFrame with ranks assigned to every explainer.\n",
    "\n",
    "    Returns:\n",
    "        dictionary of explainers with aggregated weighted metric score.\n",
    "    \"\"\"\n",
    "    # use equally weighted metrics in this example\n",
    "    if metric_weight_dict is None:\n",
    "        metric_weight_dict = {key: 1.0 for key in metrics.keys()}\n",
    "\n",
    "    # calculate final score\n",
    "    final_score = {}\n",
    "    for row in df_normalised_rank.iterrows():\n",
    "        weighted_score = sum([weight * row[1].loc[key] for key, weight in metric_weight_dict.items()])\n",
    "        final_score[row[0]] = weighted_score\n",
    "\n",
    "    sorted_explaienr_list = sorted(final_score.items(), key=lambda x:x[1], reverse=True)\n",
    "    return sorted_explaienr_list\n",
    "\n",
    "metric_weight_dict = {key: 1.0 for key in metrics.keys()}\n",
    "best_explainer = get_explainers_score(\n",
    "    df_normalised_rank=df_normalised_rank,\n",
    "    metric_weight_dict=metric_weight_dict,\n",
    ")\n",
    "\n",
    "print(\"Best explainers based on weighted metric score (higher is better)\")\n",
    "for explainer in best_explainer:\n",
    "    print(f\"\\tExplainer_name: '{explainer[0]}', score: {explainer[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics parametrization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the spyder graph you can compare different XAI algorithms based on provided metrics. In this example we have used 3 metrics with and without normalization to show you that it's important to take parametrization into account as the results may differ significantly. For `Sparseness` and `Complexity` normalisation is not a problem but for `Effective Complexity` it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spyder_graph(\n",
    "    df_normalised_rank=df_normalised_rank,\n",
    "    xai_methods=xai_methods,\n",
    "    metric_weight_dict=metric_weight_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_wrapper = {\n",
    "    \"IntegratedGradients\": integrated_gradients_explainer,\n",
    "    \"GradientSHAP\": gradient_shap_explainer,\n",
    "}\n",
    "\n",
    "# Define XAI methods and metrics.\n",
    "xai_methods = list(explainer_wrapper.keys())\n",
    "metrics = {\n",
    "    \"NonSensitivity\": quantus.NonSensitivity(\n",
    "        n_samples=100,    # number of samples to generate\n",
    "        features_in_step=512, # number of features permuted in each step; the higher number the faster each step is computed\n",
    "        aggregate_func=np.mean,\n",
    "        return_aggregate=True,\n",
    "        disable_warnings=False,\n",
    "        display_progressbar=True,   # important to see progress bar and estimate time of computations\n",
    "    ),\n",
    "    \"Sparseness\": quantus.Sparseness(\n",
    "        abs=True,\n",
    "        normalise=False,\n",
    "        aggregate_func=np.mean,\n",
    "        return_aggregate=True,\n",
    "        disable_warnings=False,\n",
    "    ),\n",
    "    \"Complexity\": quantus.Complexity(\n",
    "        abs=True,\n",
    "        normalise=False,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_attributes = compute_metrics_for_explainers(\n",
    "    x_batch_list=x_batch_list,\n",
    "    y_batch_list=y_batch_list,\n",
    "    explainer_wrapper=explainer_wrapper,\n",
    "    model=model,\n",
    "    layer=layer,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compute_metrics(\n",
    "    xai_methods=xai_methods,\n",
    "    x_batch=x_batch,\n",
    "    y_batch=y_batch,\n",
    "    explainer_attributes=explainer_attributes,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "df_normalised_rank = calculate_metric_ranks(\n",
    "    metrics=metrics,\n",
    "    xai_methods=xai_methods,\n",
    "    results=results,\n",
    "    lower_is_better_metric_names=[\n",
    "        \"Sparseness\",\n",
    "    ],\n",
    ")\n",
    "df_normalised_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_weight_dict = {key: 1.0 for key in metrics.keys()}\n",
    "best_explainer = get_explainers_score(\n",
    "    df_normalised_rank=df_normalised_rank,\n",
    "    metric_weight_dict=metric_weight_dict,\n",
    ")\n",
    "\n",
    "print(\"Best explainers based on weighted metric score (higher is better)\")\n",
    "for explainer in best_explainer:\n",
    "    print(f\"\\tExplainer_name: '{explainer[0]}', score: {explainer[1]}\")\n",
    "\n",
    "plot_spyder_graph(\n",
    "    df_normalised_rank=df_normalised_rank,\n",
    "    xai_methods=xai_methods,\n",
    "    metric_weight_dict=metric_weight_dict,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "7f79331cdf6e3c8bfe168cb4666a98099574090ffd65bf1303d625797507a1f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
