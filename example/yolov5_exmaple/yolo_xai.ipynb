{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO_v5 XAI example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final, List, Tuple\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from PIL import Image\n",
    "import IPython\n",
    "\n",
    "from autoxai.context_manager import AutoXaiExplainer, Explainers, ExplainerWithParams\n",
    "from autoxai.explainer.base_explainer import CVExplainer\n",
    "from example.yolov5_exmaple.yolo_utils import (  # scale_boxes,\n",
    "    get_variables,\n",
    "    letterbox,\n",
    "    make_divisible,\n",
    "    non_max_suppression,\n",
    "    xywh2xyxy,\n",
    "    MAXIMUM_BBOX_WIDTH_HEIGHT,\n",
    "    MAXIMUM_NUMBER_OF_BOXES_TO_NMS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define target class to be explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET: Final[int] = 0\n",
    "\"\"\" The target class to be explained with XAI.\n",
    "\n",
    "For yolo it takes all preditctions belonging to the given class.\n",
    "It means that if 2 persons were detected, the XAI will be computed\n",
    "for both of them. Instance specific XAI requires code modification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Craete model and open sample image.\n",
    "\n",
    "Downaload YOLO model from torchhub and open sample image.\n",
    "Check for CUDA device availability and move the model to CUDA if available.\n",
    "Get number of classes and stride, used by the given YOLO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", pretrained=True)\n",
    "image = Image.open(os.path.join(os.path.dirname(os.getcwd()),\"images/zidane.jpg\"))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device=device)\n",
    "params = dict(get_variables(model=model, include=(\"names\", \"stride\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom YOLO wrapper for XAI\n",
    "\n",
    "By default YOLO model downloaded from torchhub is already a wrapper on the YOLO model.\n",
    "It has model.model(...), which performs the regular YOLO prediction and custom model(...) method (forward(...) method), which do following steps:  \n",
    "- image pre-processing (image, resizing, scaling, padding and normalization)\n",
    "- model.model(...) call YOLO model on the pre-processed image\n",
    "- non-max suppression algorithm, that removes duplicated bounding boxes and predictions with low confidence\n",
    "- output bounding-box scaling\n",
    "- visualization optional\n",
    "\n",
    "The output of the model(...) function is already postprocessed and contains only classes with high confidence and reduced number of bboxes.\n",
    "\n",
    "YOLO models output.shape:\n",
    "\n",
    "a) model(...)  \n",
    "* if we feed the `torch.Tensor` it only runs model.model(...) and the output is of shape = <span style=\"color:orange\">[B,15120,85].</span>, where 15120 are all object predictions, and 85 is:\n",
    "    * bbox = [:4]\n",
    "    * object confidence [5]\n",
    "    * class confidence [6:] ,(80 classes)\n",
    "* if we feed an numpy/PIL image it runs the whole pre- and post-processing pipeline described above.  The output object is of class `model.common.Detections` and contains multiple fileds like `pred:List[torch.Tensor] = [[num_detections,6]]`, `xywh:List[torch.Tensor] = [[num_detections,6]]`,`xyxy:List[torch.Tensor] = [[num_detections,6]]` and other fields. `6` is an output of the non-max suppression algorithm used in YOLO. [:4] are bboxes, [5] is confidence and [6] is class number.\n",
    "\n",
    "b) model.model(...)\n",
    "* we get same results as running model(...) with `torch.Tensor`. See above.\n",
    "\n",
    "\n",
    "**Most XAI algorithms require however output to be of shape:** <span style=\"color:orange\">[B,number_of_classes]</span>\n",
    "\n",
    ", where B is a  batch. It has also to be fully differentiable.\n",
    "\n",
    "In order to get explanation for YOLO model we need to modify the YOLO model output shape `[B,1520,85] -> [B,80]`. The `XaiYoloWrapper` below does exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XaiYoloWrapper(torch.nn.Module):\n",
    "    \"\"\"The Xai wrapper for the yolo model.\n",
    "\n",
    "    Most explainers except the model output to consist\n",
    "    only classes. However many models have custom outputs.\n",
    "    In case of YOLO_v5 the output is [N,6] tensor, where:\n",
    "        - N: is the number of predicted objects\n",
    "        - [x,y,w,h,confidence,class]\n",
    "\n",
    "    In order to make the output usable by regular\n",
    "    xai expaliners, we need to convert the output\n",
    "    to the following shape:\n",
    "        - [cls1, cls2, ....., clsM], where M is\n",
    "        the total number of classes.\n",
    "\n",
    "    We loose the information about number of predictions\n",
    "    and thier locations. In order to get those data\n",
    "    we need to run the regular inference separately.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model, conf: float = 0.25, iou: float = 0.45\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: the yolo model to be used.\n",
    "            conf: confidence threshold for predicted objects\n",
    "            iou: iou threshold for preddicted bboxes for nms algorithm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.training = model.training\n",
    "\n",
    "        params = dict(get_variables(model=model, include=(\"names\")))\n",
    "        self.number_of_classes: int = len(params[\"names\"])\n",
    "        self.conf: float = conf\n",
    "        self.iou: float = iou\n",
    "\n",
    "    def xai_non_max_suppression(\n",
    "        self,\n",
    "        prediction: torch.Tensor,\n",
    "        conf_thres: float = 0.25,\n",
    "        iou_thres: float = 0.45,\n",
    "        agnostic: bool = False,\n",
    "        max_det: int = 300,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Non-Maximum Suppression (NMS) on inference results to reject overlapping detections\n",
    "\n",
    "        Args:\n",
    "            prediction: the model prediction\n",
    "            conf_thres: confidence threshold, for counting the detection as valid\n",
    "            iou_thres: intersection over union threshold for non max suppresion algorithm\n",
    "            agnostic: if True, non max suppression algorithm is run on raw bboxes. However it may\n",
    "                happen that different classes have bboxes in similar place. NMS would discard one of\n",
    "                those bboxes and keep only the one with higher confidence. If we want to keep bboxes\n",
    "                that are in similar place, but have different class label, we should set agnostic to False.\n",
    "            max_det: maximum number of detections\n",
    "\n",
    "        Returns:\n",
    "            batch of detections of shape (B,80), where 80 are classes confidence\n",
    "\n",
    "        Example output:\n",
    "                cls0_conf   cls1_conf   ....    cls79_conf\n",
    "            0  0.005       0.00002     ...     0.87002\n",
    "            1  0.535       0.20002     ...     0.08002\n",
    "            .  ...         ...         ...     ...\n",
    "            B  0.00008     0.10302     ...     0.0289\n",
    "        \"\"\"\n",
    "\n",
    "        # Checks\n",
    "        assert (\n",
    "            0 <= conf_thres <= 1\n",
    "        ), f\"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0\"\n",
    "        assert (\n",
    "            0 <= iou_thres <= 1\n",
    "        ), f\"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0\"\n",
    "        if isinstance(\n",
    "            prediction, (list, tuple)\n",
    "        ):  # YOLOv5 model in validation model, output = (inference_out, loss_out)\n",
    "            prediction = prediction[0]  # select only inference output\n",
    "\n",
    "        device = prediction.device\n",
    "        mps = \"mps\" in device.type  # Apple MPS\n",
    "        if mps:  # MPS not fully supported yet, convert tensors to CPU before NMS\n",
    "            prediction = prediction.cpu()\n",
    "\n",
    "        # calculate batch size\n",
    "        batch_size = prediction.shape[0]  # batch size\n",
    "\n",
    "        # calculate True/False list for all predictions that meet confidence threshold\n",
    "        # criteria for all samples in batch\n",
    "        xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "        # set start mask index as first element after the last class index\n",
    "        mi = 5 + self.number_of_classes  # mask start index\n",
    "\n",
    "        # create empty tensor representing result - probability of each class\n",
    "        out_predictions = torch.zeros(\n",
    "            (batch_size, self.number_of_classes), device=prediction.device\n",
    "        )\n",
    "        class_confidence_range = range(5, mi)\n",
    "\n",
    "        # pylint: disable = unnecessary-comprehension\n",
    "        for xi, _ in enumerate([jj for jj in prediction]):\n",
    "\n",
    "            # get sample prediction\n",
    "            x = prediction[xi]\n",
    "\n",
    "            # get all anchors that meet confidence threshold criteria from a single sample from a batch\n",
    "            x_high_conf = x.detach()[xc[xi]]\n",
    "\n",
    "            # if none of the anchors meet threshold criteria\n",
    "            if not x_high_conf.shape[0]:\n",
    "                # get class outputs\n",
    "                x = x[:, class_confidence_range]\n",
    "                # set all class outputs to zero (no gradient)\n",
    "                # does outputs does not contribute to the final prediction\n",
    "                x *= 0\n",
    "                # sum same classes together to get the shape [number_of_objectes,num_of_classes] -> [num_of_classes]\n",
    "                out_predictions[xi] = x.sum(dim=0)\n",
    "                continue\n",
    "\n",
    "            # multiply class probability by confidence score\n",
    "            x_high_conf[:, 5:] *= x_high_conf[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "            # get bounding box dimensions\n",
    "            box = xywh2xyxy(x_high_conf[:, :4])\n",
    "\n",
    "            # get confidence and argmax of classes for all anchors\n",
    "            conf, j = x_high_conf[:, class_confidence_range].max(1, keepdim=True)\n",
    "\n",
    "            # overwrite anchors that meet confidence criteria with\n",
    "            # bounding box dimensions, confidence, probability and\n",
    "            x_high_conf = torch.cat((box, conf, j.float()), dim=1)[\n",
    "                conf.view(-1) > conf_thres\n",
    "            ]\n",
    "\n",
    "            # get number of anchors\n",
    "            number_of_anchors = x_high_conf.shape[0]  # number of boxes\n",
    "\n",
    "            # if no anchors are present\n",
    "            if not number_of_anchors:  # no boxes\n",
    "                # get only class confidence\n",
    "                x = x[:, 5:]\n",
    "                # set all class outputs to zero (no gradient)\n",
    "                # does outputs does not contribute to the final prediction\n",
    "                x *= 0\n",
    "                # sum same classes together to get the shape [number_of_objectes,num_of_classes] -> [num_of_classes]\n",
    "                out_predictions[xi] = x.sum(dim=0)\n",
    "                continue\n",
    "\n",
    "            # get indices of predictions by confidence score in descending order\n",
    "            x_indexs = x_high_conf[:, 4].argsort(descending=True)[:MAXIMUM_NUMBER_OF_BOXES_TO_NMS]\n",
    "\n",
    "            # get predictions by confidence in descending order\n",
    "            x_high_conf = x_high_conf[x_indexs]\n",
    "\n",
    "            # get class\n",
    "            c = x_high_conf[:, 5:6] * (0 if agnostic else MAXIMUM_BBOX_WIDTH_HEIGHT)  # classes\n",
    "            # get boxes (with offset by class) and scores\n",
    "            boxes, scores = (\n",
    "                x_high_conf[:, :4] + c,\n",
    "                x_high_conf[:, 4],\n",
    "            )\n",
    "\n",
    "            # get bounding boxes indices to keep from NMS\n",
    "            selected_indices = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "\n",
    "            # limit detections to specified number\n",
    "            selected_indices = selected_indices[:max_det]  # limit detections\n",
    "\n",
    "            # get indexes of x_high_conf tensor, with high confidence and non-overlaping bboxes\n",
    "            x_indexs = x_indexs[selected_indices]\n",
    "\n",
    "            # grab indexes x  tensor, with high confidence and non-overlaping bboxes\n",
    "            pick_indices = xc[xi].nonzero()[x_indexs]\n",
    "\n",
    "            # get classes confidence\n",
    "            # in place opeartions not supported for gradient computation\n",
    "            # we need to clone the tensor and keep track of gradient\n",
    "            class_confidence = x[:, class_confidence_range].clone()\n",
    "            if class_confidence.requires_grad:\n",
    "                class_confidence.retain_grad()\n",
    "            # get object confidence\n",
    "            object_confidence = x[:, 4:5].clone()\n",
    "            if object_confidence.requires_grad:\n",
    "                object_confidence.retain_grad()\n",
    "\n",
    "            # multiply class confidence by object confidence\n",
    "            x[:, class_confidence_range] = class_confidence * object_confidence\n",
    "\n",
    "            # retain only classes predictions\n",
    "            x = x[:, class_confidence_range]\n",
    "\n",
    "            # create mask of anchors and mark selected\n",
    "            mask = torch.zeros_like(x)\n",
    "            mask[pick_indices] = 1\n",
    "\n",
    "            # erase non-selected anchors\n",
    "            x = x * mask\n",
    "\n",
    "            # sum probabilities of classes over all anchors\n",
    "            # instance confidence to semantic confidence\n",
    "            out_predictions[xi] = x.sum(dim=0)\n",
    "\n",
    "        return out_predictions\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.model(x)\n",
    "        x = self.xai_non_max_suppression(\n",
    "            x,\n",
    "            conf_thres=self.conf,\n",
    "            iou_thres=self.iou,\n",
    "            agnostic=False,\n",
    "            max_det=1000,\n",
    "        )  # NMS\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a wrapped model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = XaiYoloWrapper(model=model.model, conf=model.conf, iou=model.iou).to(\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pre-process image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(\n",
    "    image: np.ndarray, sample_model_parameter: torch.Tensor, stride: int\n",
    ") -> Tuple[torch.Tensor, List[int], List[int]]:\n",
    "    \"\"\"Transform the input image to the yolo network.\n",
    "\n",
    "    Args:\n",
    "        image: the input image to the network\n",
    "        sample_model_parameter: the model parameter is used to read\n",
    "            the model type (fp32/fp16) and target device\n",
    "        stride: the yolo network stride\n",
    "\n",
    "    Retuns:\n",
    "        tensor image ready to be feed into the network\n",
    "    \"\"\"\n",
    "    size: Tuple[int, int] = (640, 640)\n",
    "    if image.shape[0] < 5:  # image in CHW\n",
    "        image = image.transpose((1, 2, 0))  # reverse dataloader .transpose(2, 0, 1)\n",
    "    image = (\n",
    "        image[..., :3] if image.ndim == 3 else cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    )  # enforce 3ch input\n",
    "    shape0 = image.shape[:2]  # HWC\n",
    "    g = max(size) / max(shape0)  # gain\n",
    "    shape1 = [int(y * g) for y in shape0]\n",
    "    np_image = image if image.data.contiguous else np.ascontiguousarray(image)  # update\n",
    "    shape1 = [make_divisible(x, stride) for x in np.array(shape1)]  # inf shape\n",
    "    x = letterbox(np_image, shape1, auto=False)[0]  # pad\n",
    "    x = np.ascontiguousarray(\n",
    "        np.expand_dims(np.array(x), axis=0).transpose((0, 3, 1, 2))\n",
    "    )  # stack and BHWC to BCHW\n",
    "    x = (\n",
    "        torch.from_numpy(x)\n",
    "        .to(sample_model_parameter.device)\n",
    "        .type_as(sample_model_parameter)\n",
    "        / 255\n",
    "    )  # uint8 to fp16/32\n",
    "\n",
    "    return x, shape0, shape1\n",
    "\n",
    "input_image, _, _ = pre_process(\n",
    "    image=np.asarray(image),\n",
    "    sample_model_parameter=next(model.parameters()),\n",
    "    stride=params[\"stride\"],\n",
    ")\n",
    "input_image = input_image.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with AutoXaiExplainer(\n",
    "    model=yolo_model,\n",
    "    explainers=[\n",
    "        ExplainerWithParams(\n",
    "            explainer_name=Explainers.CV_LAYER_GRADCAM_EXPLAINER,\n",
    "            target=0,\n",
    "        )\n",
    "    ],\n",
    ") as xai_model:\n",
    "    _, attributions = xai_model(input_image)\n",
    "\n",
    "attributions = attributions[\"CV_LAYER_GRADCAM_EXPLAINER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run regular inference\n",
    "\n",
    "Because the XAI was ran using wrapped model, the inference results from XAI are of shape `[B,80]`. The information about bboxes as well as about each instance is lost. We have only information about semantic classes detected in an image. Therefore in order to get instance level information with bboxes, we need to run the regular model once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard inference\n",
    "y = model.model(input_image)\n",
    "y = non_max_suppression(\n",
    "    y,\n",
    "    conf_thres=model.conf,\n",
    "    iou_thres=model.iou,\n",
    "    classes=None,\n",
    "    agnostic=False,\n",
    "    multi_label=False,\n",
    "    max_det=1000,\n",
    ")  # NMS\n",
    "\n",
    "# uncomment in order to scale boxes to the original image size\n",
    "# scale_boxes(shape1, y[0][:, :4], shape0)\n",
    "y = y[0].detach().cpu()\n",
    "bboxes = y[:, :4]\n",
    "normalized_image: torch.Tensor = (\n",
    "    (\n",
    "        (input_image - torch.min(input_image))\n",
    "        / (torch.max(input_image) - torch.min(input_image))\n",
    "        * 255\n",
    "    )\n",
    "    .type(torch.uint8)\n",
    "    .squeeze()\n",
    ")\n",
    "labels: List[str] = [params[\"names\"][label.item()] for label in y[:, -1]]\n",
    "pred_image = torchvision.utils.draw_bounding_boxes(\n",
    "    image=normalized_image,\n",
    "    boxes=bboxes,\n",
    "    labels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = CVExplainer.visualize(\n",
    "    attributions=attributions.squeeze(), transformed_img=pred_image\n",
    ")\n",
    "canvas = FigureCanvas(figure)\n",
    "canvas.draw()\n",
    "buf = canvas.buffer_rgba()\n",
    "image = np.asarray(buf)\n",
    "cv2.putText(\n",
    "    img=image,\n",
    "    text=params[\"names\"][TARGET],\n",
    "    org=(image.shape[0] // 2, image.shape[1] // 8),\n",
    "    fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    fontScale=image.shape[0] / 500,\n",
    "    color=(0, 0, 0),\n",
    ")\n",
    "IPython.display.display(Image.fromarray(image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b5cbf844328a2b2453e308667fce144df58c2e2f312b02199ed467ff7741f51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
